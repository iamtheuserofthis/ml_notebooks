{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ENSEMBLE LEARNING\n",
    "A group of predictors is called an __ensemble__\n",
    "\n",
    "Aggregating the predictions of a group of predictors is __Ensemble Learning__.\n",
    "\n",
    "An Ensemble Learning algorithm is called an __Ensemble method__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifiers\n",
    "\n",
    "<img src=\"../notes_images/ensemble1.png\" width=400 height=350>\n",
    "\n",
    "A very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. This majority-vote classifier is called a hard voting classifier.\n",
    "\n",
    "Voting classifier often achieves a higher accuracy than the best classifier in the ensemble. In fact, even if each classifier is a weak learner (meaning it does only slightly better than random guessing), the ensemble can still be a strong learner (achieving high accuracy), provided there are a sufficient number of weak learners and they are sufficiently diverse.\n",
    "\n",
    "\n",
    "#### Hard Voting Classifier\n",
    "A very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. This majority-vote classifier is called a hard voting classifier\n",
    "\n",
    "#### Soft Voting Classifier\n",
    "If all classifiers are able to estimate class probabilities (i.e., they have a predict_proba() method), then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called soft voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of voting classifier\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
       "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
       "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
       "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
       "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  target  \n",
       "0                            3.92   1065.0       0  \n",
       "1                            3.40   1050.0       0  \n",
       "2                            3.17   1185.0       0  \n",
       "3                            3.45   1480.0       0  \n",
       "4                            2.93    735.0       0  \n",
       "..                            ...      ...     ...  \n",
       "173                          1.74    740.0       2  \n",
       "174                          1.56    750.0       2  \n",
       "175                          1.56    835.0       2  \n",
       "176                          1.62    840.0       2  \n",
       "177                          1.60    560.0       2  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data['data'])\n",
    "df.columns = data['feature_names']\n",
    "df['target'] = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,:len(df.columns)-1],df.iloc[:,len(df.columns)-1], test_size=0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scalar', StandardScaler()),\n",
       "                ('vot_clf',\n",
       "                 VotingClassifier(estimators=[('lc', LogisticRegression()),\n",
       "                                              ('rc', RandomForestClassifier()),\n",
       "                                              ('svc',\n",
       "                                               SVC(C=100, probability=True))],\n",
       "                                  voting='soft'))])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC(kernel='rbf', C=100, probability=True)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "estimators = [('lc',log_clf), ('rc', rnd_clf), ('svc', svm_clf)],\n",
    "    voting= 'soft'#'hard'\n",
    ")\n",
    "\n",
    "pipe_clf = Pipeline(\n",
    "[('scalar', StandardScaler()),('vot_clf',voting_clf)])\n",
    "\n",
    "pipe_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model._logistic.LogisticRegression'> 0.9830508474576272\n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'> 0.9830508474576272\n",
      "<class 'sklearn.svm._classes.SVC'> 0.9830508474576272\n",
      "<class 'sklearn.pipeline.Pipeline'> 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, pipe_clf):\n",
    "    scalar = StandardScaler()\n",
    "    X_train_scaled =  scalar.fit_transform(X_train)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    X_test_scaled = scalar.transform(X_test)\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    print(clf.__class__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVC(kernel='rbf', C=100, probability=True)\n",
    "svm_classifier.fit(X_train,y_train)\n",
    "\n",
    "y_pred = svm_classifier.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01196076, 0.83379765, 0.15424159],\n",
       "       [0.00662472, 0.92475823, 0.06861706],\n",
       "       [0.01787197, 0.71416273, 0.2679653 ],\n",
       "       [0.84252593, 0.12780008, 0.02967399],\n",
       "       [0.40475602, 0.21024496, 0.38499902],\n",
       "       [0.02523919, 0.68669689, 0.28806392],\n",
       "       [0.08594689, 0.33031732, 0.58373579],\n",
       "       [0.00998081, 0.84421051, 0.14580868],\n",
       "       [0.87523394, 0.06930813, 0.05545793],\n",
       "       [0.00524639, 0.93589352, 0.05886009],\n",
       "       [0.74942495, 0.24910006, 0.00147499],\n",
       "       [0.76831637, 0.15446479, 0.07721884],\n",
       "       [0.00695286, 0.87447781, 0.11856933],\n",
       "       [0.82257576, 0.1730821 , 0.00434214],\n",
       "       [0.05830503, 0.38513649, 0.55655848],\n",
       "       [0.00698239, 0.90894507, 0.08407254],\n",
       "       [0.90217358, 0.048742  , 0.04908442],\n",
       "       [0.00504048, 0.92265245, 0.07230707],\n",
       "       [0.12264159, 0.32539103, 0.55196738],\n",
       "       [0.17153853, 0.22585115, 0.60261032],\n",
       "       [0.88757684, 0.09518146, 0.0172417 ],\n",
       "       [0.83675168, 0.12057407, 0.04267425],\n",
       "       [0.07424885, 0.39611485, 0.5296363 ],\n",
       "       [0.91231501, 0.08301438, 0.00467061],\n",
       "       [0.0181309 , 0.73724966, 0.24461945],\n",
       "       [0.83935572, 0.15534191, 0.00530237],\n",
       "       [0.04358248, 0.50282628, 0.45359124],\n",
       "       [0.035592  , 0.56450732, 0.39990067],\n",
       "       [0.87446635, 0.09167197, 0.03386168],\n",
       "       [0.01581013, 0.74694335, 0.23724652],\n",
       "       [0.05250816, 0.47409333, 0.47339851],\n",
       "       [0.15008473, 0.26110377, 0.5888115 ],\n",
       "       [0.84132216, 0.13914851, 0.01952933],\n",
       "       [0.07516931, 0.41184737, 0.51298331],\n",
       "       [0.54739781, 0.15458263, 0.29801956],\n",
       "       [0.02474158, 0.6297446 , 0.34551382],\n",
       "       [0.88678546, 0.10982859, 0.00338595],\n",
       "       [0.58430931, 0.160467  , 0.25522369],\n",
       "       [0.04231056, 0.50407964, 0.4536098 ],\n",
       "       [0.00632518, 0.90375054, 0.08992429],\n",
       "       [0.80173375, 0.10562311, 0.09264315],\n",
       "       [0.0077986 , 0.87927339, 0.11292801],\n",
       "       [0.04141898, 0.48190016, 0.47668085],\n",
       "       [0.00324217, 0.96005572, 0.03670211],\n",
       "       [0.02901912, 0.65338745, 0.31759344],\n",
       "       [0.09608594, 0.28633699, 0.61757707],\n",
       "       [0.84362887, 0.15006009, 0.00631104],\n",
       "       [0.46638219, 0.16263204, 0.37098577],\n",
       "       [0.02802819, 0.64006432, 0.33190749],\n",
       "       [0.04771795, 0.53169989, 0.42058216],\n",
       "       [0.88839841, 0.10766324, 0.00393834],\n",
       "       [0.08071689, 0.34501644, 0.57426667],\n",
       "       [0.02804487, 0.61668295, 0.35527218],\n",
       "       [0.00338187, 0.95720541, 0.03941272],\n",
       "       [0.01617062, 0.74608793, 0.23774145],\n",
       "       [0.1458888 , 0.27477904, 0.57933216],\n",
       "       [0.01668082, 0.74263863, 0.24068054],\n",
       "       [0.86423796, 0.12367349, 0.01208854],\n",
       "       [0.0437938 , 0.51864646, 0.43755974]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'y':pipe_clf.predict(X_test), 'y_true':y_test})\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.50002901e-04 9.98906359e-01 6.43638144e-04]\n",
      " [5.55700469e-04 9.99260716e-01 1.83583303e-04]\n",
      " [1.95797421e-03 5.41542916e-04 9.97500483e-01]\n",
      " [9.21570906e-01 6.55610942e-02 1.28679997e-02]\n",
      " [8.99291499e-01 9.93767934e-02 1.33170810e-03]\n",
      " [3.63437539e-04 9.98499363e-01 1.13719918e-03]\n",
      " [2.38825452e-03 1.38173738e-04 9.97473572e-01]\n",
      " [2.08627583e-04 9.99351280e-01 4.40092573e-04]\n",
      " [4.66097296e-01 5.33198557e-01 7.04146920e-04]\n",
      " [2.90825716e-03 9.96917407e-01 1.74335788e-04]\n",
      " [9.99961516e-01 4.73818703e-06 3.37456057e-05]\n",
      " [9.14234330e-01 7.41631603e-02 1.16025098e-02]\n",
      " [2.89766066e-04 2.97111083e-03 9.96739123e-01]\n",
      " [9.97794175e-01 1.47804298e-03 7.27781766e-04]\n",
      " [3.07784620e-03 4.52739233e-04 9.96469415e-01]\n",
      " [6.32474347e-03 9.85906907e-01 7.76834969e-03]\n",
      " [5.35898389e-01 4.63634986e-01 4.66625595e-04]\n",
      " [1.77274719e-02 9.81981476e-01 2.91052625e-04]\n",
      " [1.27284167e-01 8.67725178e-01 4.99065517e-03]\n",
      " [3.48211211e-03 2.31106289e-04 9.96286782e-01]\n",
      " [9.94439486e-01 2.99971613e-03 2.56079800e-03]\n",
      " [9.99479495e-01 4.40573556e-04 7.99318103e-05]\n",
      " [5.73282773e-03 7.17920754e-02 9.22475097e-01]\n",
      " [9.97513975e-01 1.43610050e-03 1.04992452e-03]\n",
      " [2.12887698e-04 9.99678159e-01 1.08953781e-04]\n",
      " [9.45094808e-01 4.72871968e-02 7.61799480e-03]\n",
      " [5.70824654e-04 4.25328993e-02 9.56896276e-01]\n",
      " [3.60880769e-04 9.99420412e-01 2.18706845e-04]\n",
      " [9.95227643e-01 3.16557107e-03 1.60678556e-03]\n",
      " [7.39294916e-04 9.88958404e-01 1.03023008e-02]\n",
      " [5.52145360e-04 9.99058193e-01 3.89662101e-04]\n",
      " [9.41403252e-04 1.03596153e-03 9.98022635e-01]\n",
      " [9.95494943e-01 4.38399882e-03 1.21058509e-04]\n",
      " [3.26803156e-02 9.36426156e-01 3.08935282e-02]\n",
      " [9.97791885e-01 1.43587737e-03 7.72237340e-04]\n",
      " [5.97703714e-04 1.16196750e-02 9.87782621e-01]\n",
      " [9.99288010e-01 1.65946211e-04 5.46043538e-04]\n",
      " [8.54321645e-01 1.43934838e-01 1.74351682e-03]\n",
      " [4.61586271e-03 4.54141445e-02 9.49969993e-01]\n",
      " [3.43084744e-03 9.96046612e-01 5.22540461e-04]\n",
      " [9.98516653e-01 9.87847567e-04 4.95499019e-04]\n",
      " [7.49524285e-05 9.99761167e-01 1.63880553e-04]\n",
      " [6.78921871e-04 8.03751885e-05 9.99240703e-01]\n",
      " [4.64885451e-04 9.98116545e-01 1.41856969e-03]\n",
      " [2.42469527e-02 9.72965560e-01 2.78748766e-03]\n",
      " [8.61827053e-03 9.15881272e-05 9.91290141e-01]\n",
      " [9.99276841e-01 6.11326238e-04 1.11832405e-04]\n",
      " [4.36712110e-02 9.56162358e-01 1.66430805e-04]\n",
      " [9.97696972e-04 7.12698214e-02 9.27732482e-01]\n",
      " [1.48078848e-03 6.02187360e-04 9.97917024e-01]\n",
      " [9.98610858e-01 1.78431470e-04 1.21071030e-03]\n",
      " [1.05634292e-03 1.11485772e-03 9.97828799e-01]\n",
      " [8.80585597e-04 4.40180557e-02 9.55101359e-01]\n",
      " [1.10601511e-03 9.98197833e-01 6.96152261e-04]\n",
      " [1.28136352e-03 9.87380595e-01 1.13380416e-02]\n",
      " [2.40583899e-02 1.06674791e-02 9.65274131e-01]\n",
      " [4.49422537e-02 9.54986631e-01 7.11150768e-05]\n",
      " [9.97200735e-01 2.39675955e-03 4.02505819e-04]\n",
      " [3.13320764e-04 9.99454942e-01 2.31737475e-04]]\n",
      "[[0.04 0.92 0.04]\n",
      " [0.01 0.98 0.01]\n",
      " [0.01 0.02 0.97]\n",
      " [0.88 0.08 0.04]\n",
      " [0.64 0.33 0.03]\n",
      " [0.03 0.66 0.31]\n",
      " [0.   0.   1.  ]\n",
      " [0.   1.   0.  ]\n",
      " [0.73 0.27 0.  ]\n",
      " [0.02 0.98 0.  ]\n",
      " [0.99 0.01 0.  ]\n",
      " [0.81 0.14 0.05]\n",
      " [0.   0.11 0.89]\n",
      " [0.89 0.11 0.  ]\n",
      " [0.14 0.29 0.57]\n",
      " [0.02 0.96 0.02]\n",
      " [0.3  0.7  0.  ]\n",
      " [0.   0.97 0.03]\n",
      " [0.17 0.8  0.03]\n",
      " [0.07 0.14 0.79]\n",
      " [0.96 0.02 0.02]\n",
      " [1.   0.   0.  ]\n",
      " [0.   0.29 0.71]\n",
      " [0.94 0.06 0.  ]\n",
      " [0.   1.   0.  ]\n",
      " [0.86 0.14 0.  ]\n",
      " [0.   0.15 0.85]\n",
      " [0.01 0.99 0.  ]\n",
      " [0.99 0.01 0.  ]\n",
      " [0.   0.94 0.06]\n",
      " [0.   0.99 0.01]\n",
      " [0.   0.   1.  ]\n",
      " [0.89 0.11 0.  ]\n",
      " [0.06 0.8  0.14]\n",
      " [0.98 0.01 0.01]\n",
      " [0.01 0.2  0.79]\n",
      " [0.99 0.01 0.  ]\n",
      " [0.92 0.08 0.  ]\n",
      " [0.   0.12 0.88]\n",
      " [0.13 0.75 0.12]\n",
      " [1.   0.   0.  ]\n",
      " [0.   1.   0.  ]\n",
      " [0.   0.06 0.94]\n",
      " [0.   0.99 0.01]\n",
      " [0.02 0.95 0.03]\n",
      " [0.16 0.28 0.56]\n",
      " [0.96 0.04 0.  ]\n",
      " [0.04 0.93 0.03]\n",
      " [0.   0.11 0.89]\n",
      " [0.04 0.22 0.74]\n",
      " [1.   0.   0.  ]\n",
      " [0.   0.   1.  ]\n",
      " [0.   0.21 0.79]\n",
      " [0.   0.99 0.01]\n",
      " [0.01 0.95 0.04]\n",
      " [0.04 0.11 0.85]\n",
      " [0.29 0.67 0.04]\n",
      " [0.99 0.01 0.  ]\n",
      " [0.01 0.98 0.01]]\n",
      "[[5.30307505e-03 9.90596565e-01 4.10035995e-03]\n",
      " [1.78397394e-03 9.95663419e-01 2.55260691e-03]\n",
      " [8.76333735e-03 2.65918642e-02 9.64644798e-01]\n",
      " [9.63959993e-01 1.77963309e-02 1.82436762e-02]\n",
      " [9.19249862e-01 7.22211160e-02 8.52902217e-03]\n",
      " [3.71229900e-02 9.24156732e-01 3.87202782e-02]\n",
      " [9.99352868e-03 1.09374668e-02 9.79069005e-01]\n",
      " [4.66268760e-03 9.77070041e-01 1.82672714e-02]\n",
      " [2.02314627e-01 7.37088392e-01 6.05969806e-02]\n",
      " [5.59660229e-03 9.81932401e-01 1.24709971e-02]\n",
      " [9.43139964e-01 3.41886353e-02 2.26714007e-02]\n",
      " [8.91173662e-01 8.72430937e-02 2.15832439e-02]\n",
      " [2.13664861e-02 8.74282433e-02 8.91205271e-01]\n",
      " [9.91286337e-01 2.49141711e-03 6.22224541e-03]\n",
      " [6.21417154e-02 2.27316062e-01 7.10542223e-01]\n",
      " [3.13410724e-02 9.26909347e-01 4.17495804e-02]\n",
      " [4.19787855e-02 9.00024976e-01 5.79962387e-02]\n",
      " [1.23883491e-02 9.67629010e-01 1.99826413e-02]\n",
      " [1.53844481e-01 8.31391481e-01 1.47640374e-02]\n",
      " [1.11614249e-02 2.65276924e-02 9.62310883e-01]\n",
      " [9.80880557e-01 8.38141912e-03 1.07380234e-02]\n",
      " [9.93617093e-01 3.36501258e-03 3.01789394e-03]\n",
      " [9.57396260e-03 3.58969511e-02 9.54529086e-01]\n",
      " [9.03022202e-01 6.67741662e-02 3.02036319e-02]\n",
      " [2.34350368e-05 9.96889391e-01 3.08717375e-03]\n",
      " [9.61828616e-01 1.74878819e-02 2.06835017e-02]\n",
      " [5.32069526e-03 4.68000060e-02 9.47879299e-01]\n",
      " [6.70274269e-03 9.78881802e-01 1.44154554e-02]\n",
      " [9.87140076e-01 3.64428395e-03 9.21563966e-03]\n",
      " [2.71084501e-03 9.84601267e-01 1.26878876e-02]\n",
      " [3.48630893e-03 9.86042381e-01 1.04713098e-02]\n",
      " [1.07874585e-02 6.57152827e-02 9.23497259e-01]\n",
      " [7.54750342e-01 2.14073102e-01 3.11765558e-02]\n",
      " [8.29346312e-02 8.42538544e-01 7.45268248e-02]\n",
      " [9.92800278e-01 2.13262103e-03 5.06710102e-03]\n",
      " [5.46821541e-03 2.26970983e-02 9.71834686e-01]\n",
      " [9.85458378e-01 2.81776100e-03 1.17238613e-02]\n",
      " [9.43884891e-01 5.03950752e-02 5.72003362e-03]\n",
      " [5.97392570e-03 7.55426593e-02 9.18483415e-01]\n",
      " [6.89633917e-03 9.84116334e-01 8.98732725e-03]\n",
      " [9.89359679e-01 5.31639898e-03 5.32392171e-03]\n",
      " [2.74696913e-05 9.96236695e-01 3.73583505e-03]\n",
      " [1.36738374e-02 3.74531992e-02 9.48872963e-01]\n",
      " [5.00551895e-03 9.89727196e-01 5.26728537e-03]\n",
      " [1.34321178e-02 9.65913049e-01 2.06548334e-02]\n",
      " [9.17063178e-02 5.74223054e-01 3.34070628e-01]\n",
      " [9.86436066e-01 6.48665493e-03 7.07727907e-03]\n",
      " [3.01977110e-02 9.13639230e-01 5.61630593e-02]\n",
      " [4.81055131e-03 5.44506746e-02 9.40738774e-01]\n",
      " [1.13656554e-02 3.42484494e-02 9.54385895e-01]\n",
      " [9.82184643e-01 5.99650669e-03 1.18188500e-02]\n",
      " [2.99772769e-03 4.24097151e-03 9.92761301e-01]\n",
      " [1.82320088e-02 1.58553378e-01 8.23214613e-01]\n",
      " [2.13496511e-03 9.91401017e-01 6.46401834e-03]\n",
      " [1.60860804e-03 9.93312724e-01 5.07866784e-03]\n",
      " [7.65860874e-03 1.24224691e-02 9.79918922e-01]\n",
      " [2.62821245e-02 9.46349414e-01 2.73684614e-02]\n",
      " [9.78086220e-01 1.54559648e-02 6.45781469e-03]\n",
      " [2.92049002e-03 9.85857713e-01 1.12217974e-02]]\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC(kernel='rbf', C=100, probability=True)\n",
    "\n",
    "std_scalar = StandardScaler()\n",
    "X_train_scaled = std_scalar.fit_transform(X_train)\n",
    "log_clf.fit(X_train_scaled,y_train)\n",
    "rnd_clf.fit(X_train_scaled,y_train)\n",
    "svm_clf.fit(X_train_scaled,y_train)\n",
    "\n",
    "X_test_scaled = std_scalar.transform(X_test)\n",
    "\n",
    "print(log_clf.predict_proba(X_test_scaled))\n",
    "print(rnd_clf.predict_proba(X_test_scaled))\n",
    "print(svm_clf.predict_proba(X_test_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAGGING AND PASTING\n",
    "Using the same training algorithm for every predictor(e.g. regressor), but to train them on different random subsets of the training set. \n",
    "When sampling is performed \n",
    "1. with replacement, this method is called bagging(short forbootstrap aggregating \n",
    "2. without replacement, it is called pasting.\n",
    "\n",
    "__Bagging__ takes the model with high variance and low bias and reduces the variance without affecting the bias.\n",
    "\n",
    "__Boosting__ takes the model with low variance and high bias and reduces the bias without affecting the variance.\n",
    "\n",
    "Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. \n",
    "\n",
    "The aggregation function  typically :\n",
    "1. The statistical mode (i.e., the most frequent prediction, just like a hard voting classifier) for classification\n",
    "2. The average for regression.\n",
    "\n",
    "Each individual predictor has a higher bias than if it were trained on the original training set, but\n",
    "aggregation reduces both bias and variance.\n",
    "\n",
    "Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set.\n",
    "\n",
    "All predictors can run on different core therefore they are faster.\n",
    "\n",
    "#### Single Decision Tree vs a bagging ensemble of 500 trees\n",
    "<img src=\"../notes_images/bagging_res.png\">\n",
    "\n",
    "__It can be clearly seen that bias remains the same but the variance is reduced.__\n",
    "\n",
    "\n",
    "### Out Of Bag Evaluation\n",
    "In bagging some instances may be sampled several times for any given predictor, while others may not be sampled at all. By default a BaggingClassifier samples m training instances with replacement ( bootstrap=True ), where m is the size of the training set. This means that only about 63% of the training instances are sampled on average for each predictor. 6 The remaining 37% of the training instances that are not sampled are called out-of-bag (oob) instances. Note that they are not the same 37% for all predictors.\n",
    "\n",
    "### Random Patches and Random Subspaces\n",
    "\n",
    "The BaggingClassifier class supports sampling the features as well. This is controlled by two hyperparameters: max_features and bootstrap_features . They work the same way as max_samples and bootstrap , but for feature sampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input features.\n",
    "\n",
    "This is particularly useful when you are dealing with high-dimensional inputs (such as images). Sampling both training instances and features is called the __Random Patches__ method. Keeping all training instances (i.e., bootstrap=False and max_sam ples=1.0 ) but sampling features (i.e., bootstrap_features=True and/or max_fea\n",
    "tures smaller than 1.0) is called the __Random Subspaces method__.\n",
    "\n",
    "Trades more bias for the lower variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data['data'], columns=data['feature_names'].tolist())\n",
    "data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df,data['target'], test_size=0.20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('baggedDtreeClf',\n",
       "                 BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
       "                                   max_samples=150, n_estimators=500, n_jobs=-1,\n",
       "                                   oob_score=True))])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=150, bootstrap=True, n_jobs=-1,oob_score=True\n",
    ")\n",
    "\n",
    "pipe_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('baggedDtreeClf', bag_clf)\n",
    "])\n",
    "\n",
    "pipe_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05633803, 0.94366197],\n",
       "       [0.19774011, 0.80225989],\n",
       "       [0.92021277, 0.07978723],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00534759, 0.99465241],\n",
       "       [0.08791209, 0.91208791],\n",
       "       [0.02739726, 0.97260274],\n",
       "       [0.        , 1.        ],\n",
       "       [0.27019499, 0.72980501],\n",
       "       [0.47540984, 0.52459016],\n",
       "       [1.        , 0.        ],\n",
       "       [0.48209366, 0.51790634],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03116147, 0.96883853],\n",
       "       [0.99430199, 0.00569801],\n",
       "       [0.95786517, 0.04213483],\n",
       "       [0.47252747, 0.52747253],\n",
       "       [0.01101928, 0.98898072],\n",
       "       [0.02808989, 0.97191011],\n",
       "       [0.00533333, 0.99466667],\n",
       "       [0.06779661, 0.93220339],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.85555556, 0.14444444],\n",
       "       [0.99714286, 0.00285714],\n",
       "       [0.29096045, 0.70903955],\n",
       "       [0.94198895, 0.05801105],\n",
       "       [0.        , 1.        ],\n",
       "       [0.92307692, 0.07692308],\n",
       "       [0.20325203, 0.79674797],\n",
       "       [0.        , 1.        ],\n",
       "       [0.63114754, 0.36885246],\n",
       "       [0.01381215, 0.98618785],\n",
       "       [0.60526316, 0.39473684],\n",
       "       [1.        , 0.        ],\n",
       "       [0.27966102, 0.72033898],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99164345, 0.00835655],\n",
       "       [0.01612903, 0.98387097],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.77325581, 0.22674419],\n",
       "       [0.99142857, 0.00857143],\n",
       "       [0.01704545, 0.98295455],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.41142857, 0.58857143],\n",
       "       [0.91412742, 0.08587258],\n",
       "       [0.        , 1.        ],\n",
       "       [0.86968839, 0.13031161],\n",
       "       [0.04065041, 0.95934959],\n",
       "       [0.98847262, 0.01152738],\n",
       "       [0.13675214, 0.86324786],\n",
       "       [0.02083333, 0.97916667],\n",
       "       [1.        , 0.        ],\n",
       "       [0.21657754, 0.78342246],\n",
       "       [0.00568182, 0.99431818],\n",
       "       [0.02785515, 0.97214485],\n",
       "       [0.99393939, 0.00606061],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.73076923, 0.26923077],\n",
       "       [0.34710744, 0.65289256],\n",
       "       [0.96811594, 0.03188406],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.06060606, 0.93939394],\n",
       "       [0.0106383 , 0.9893617 ],\n",
       "       [0.53757225, 0.46242775],\n",
       "       [0.06406685, 0.93593315],\n",
       "       [0.30372493, 0.69627507],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00287356, 0.99712644],\n",
       "       [0.84365782, 0.15634218],\n",
       "       [1.        , 0.        ],\n",
       "       [0.83667622, 0.16332378],\n",
       "       [0.99186992, 0.00813008],\n",
       "       [0.82571429, 0.17428571],\n",
       "       [0.0078125 , 0.9921875 ],\n",
       "       [0.00569801, 0.99430199],\n",
       "       [0.12222222, 0.87777778],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00527704, 0.99472296],\n",
       "       [0.59942363, 0.40057637],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.66295265, 0.33704735],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.11267606, 0.88732394],\n",
       "       [0.0140056 , 0.9859944 ],\n",
       "       [0.60784314, 0.39215686],\n",
       "       [0.00550964, 0.99449036],\n",
       "       [0.17732558, 0.82267442],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00835655, 0.99164345],\n",
       "       [0.        , 1.        ],\n",
       "       [0.86285714, 0.13714286],\n",
       "       [0.23275862, 0.76724138],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00293255, 0.99706745],\n",
       "       [0.01058201, 0.98941799],\n",
       "       [0.        , 1.        ],\n",
       "       [0.12215909, 0.87784091],\n",
       "       [0.81697613, 0.18302387],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.26836158, 0.73163842],\n",
       "       [1.        , 0.        ],\n",
       "       [0.22606383, 0.77393617],\n",
       "       [0.1488764 , 0.8511236 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.24528302, 0.75471698],\n",
       "       [0.01902174, 0.98097826],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.46590909, 0.53409091],\n",
       "       [0.0109589 , 0.9890411 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.16853933, 0.83146067],\n",
       "       [0.00271003, 0.99728997],\n",
       "       [0.        , 1.        ],\n",
       "       [0.09562842, 0.90437158],\n",
       "       [0.99423631, 0.00576369],\n",
       "       [1.        , 0.        ],\n",
       "       [0.53369272, 0.46630728],\n",
       "       [0.96994536, 0.03005464],\n",
       "       [0.00289855, 0.99710145],\n",
       "       [0.00806452, 0.99193548],\n",
       "       [0.        , 1.        ],\n",
       "       [0.80978261, 0.19021739],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.94262295, 0.05737705],\n",
       "       [0.36694678, 0.63305322],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99714286, 0.00285714],\n",
       "       [0.98356164, 0.01643836],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.44722222, 0.55277778],\n",
       "       [0.56703911, 0.43296089],\n",
       "       [0.016     , 0.984     ],\n",
       "       [0.06775068, 0.93224932],\n",
       "       [0.14697406, 0.85302594],\n",
       "       [0.99147727, 0.00852273],\n",
       "       [0.4084507 , 0.5915493 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.96338028, 0.03661972],\n",
       "       [0.97457627, 0.02542373],\n",
       "       [0.83561644, 0.16438356],\n",
       "       [0.        , 1.        ],\n",
       "       [0.15819209, 0.84180791],\n",
       "       [0.86235955, 0.13764045],\n",
       "       [0.47109827, 0.52890173],\n",
       "       [0.99450549, 0.00549451],\n",
       "       [0.84033613, 0.15966387],\n",
       "       [0.60810811, 0.39189189],\n",
       "       [0.01648352, 0.98351648],\n",
       "       [0.06340058, 0.93659942],\n",
       "       [0.11699164, 0.88300836],\n",
       "       [0.00271739, 0.99728261],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.07714286, 0.92285714],\n",
       "       [0.01392758, 0.98607242],\n",
       "       [0.00842697, 0.99157303],\n",
       "       [0.00828729, 0.99171271],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.13314448, 0.86685552],\n",
       "       [0.        , 1.        ],\n",
       "       [0.44657534, 0.55342466],\n",
       "       [0.04509284, 0.95490716],\n",
       "       [0.0027027 , 0.9972973 ],\n",
       "       [0.70441989, 0.29558011],\n",
       "       [0.00557103, 0.99442897],\n",
       "       [0.98365123, 0.01634877],\n",
       "       [0.99439776, 0.00560224],\n",
       "       [0.00278552, 0.99721448],\n",
       "       [0.03098592, 0.96901408],\n",
       "       [0.07466667, 0.92533333],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01671309, 0.98328691],\n",
       "       [0.99719888, 0.00280112],\n",
       "       [1.        , 0.        ],\n",
       "       [0.77624309, 0.22375691],\n",
       "       [0.92328767, 0.07671233],\n",
       "       [0.14480874, 0.85519126],\n",
       "       [0.00842697, 0.99157303],\n",
       "       [0.08146067, 0.91853933],\n",
       "       [0.09366391, 0.90633609],\n",
       "       [0.99152542, 0.00847458],\n",
       "       [0.08169014, 0.91830986],\n",
       "       [0.00549451, 0.99450549],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99715909, 0.00284091],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.10315186, 0.89684814],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.18157182, 0.81842818],\n",
       "       [1.        , 0.        ],\n",
       "       [0.91977077, 0.08022923],\n",
       "       [0.        , 1.        ],\n",
       "       [0.4       , 0.6       ],\n",
       "       [0.01392758, 0.98607242],\n",
       "       [0.        , 1.        ],\n",
       "       [0.95854922, 0.04145078],\n",
       "       [0.17964072, 0.82035928],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9971831 , 0.0028169 ],\n",
       "       [0.05801105, 0.94198895],\n",
       "       [0.99726027, 0.00273973],\n",
       "       [0.00854701, 0.99145299],\n",
       "       [0.76487252, 0.23512748],\n",
       "       [0.99708455, 0.00291545],\n",
       "       [0.        , 1.        ],\n",
       "       [0.48087432, 0.51912568],\n",
       "       [0.88858696, 0.11141304],\n",
       "       [0.11263736, 0.88736264],\n",
       "       [0.03243243, 0.96756757],\n",
       "       [0.11965812, 0.88034188],\n",
       "       [0.16713092, 0.83286908],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95316804, 0.04683196],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.06703911, 0.93296089],\n",
       "       [0.07945205, 0.92054795],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98128342, 0.01871658],\n",
       "       [0.        , 1.        ],\n",
       "       [0.74807198, 0.25192802],\n",
       "       [0.88579387, 0.11420613],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.29810298, 0.70189702],\n",
       "       [0.52588556, 0.47411444],\n",
       "       [0.04494382, 0.95505618],\n",
       "       [0.69041096, 0.30958904],\n",
       "       [0.95652174, 0.04347826],\n",
       "       [0.01369863, 0.98630137],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00554017, 0.99445983],\n",
       "       [1.        , 0.        ],\n",
       "       [0.41416894, 0.58583106],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.21111111, 0.78888889],\n",
       "       [0.31623932, 0.68376068],\n",
       "       [0.98666667, 0.01333333],\n",
       "       [0.00283286, 0.99716714],\n",
       "       [0.15598886, 0.84401114],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.16111111, 0.83888889],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02785515, 0.97214485],\n",
       "       [0.86522911, 0.13477089],\n",
       "       [0.0084507 , 0.9915493 ],\n",
       "       [0.01358696, 0.98641304],\n",
       "       [0.73469388, 0.26530612],\n",
       "       [0.07297297, 0.92702703],\n",
       "       [0.17765043, 0.82234957],\n",
       "       [0.01902174, 0.98097826],\n",
       "       [0.24590164, 0.75409836],\n",
       "       [0.00262467, 0.99737533],\n",
       "       [0.11977716, 0.88022284],\n",
       "       [0.        , 1.        ],\n",
       "       [0.13866667, 0.86133333],\n",
       "       [0.72881356, 0.27118644],\n",
       "       [0.99721448, 0.00278552],\n",
       "       [0.40804598, 0.59195402],\n",
       "       [0.0078329 , 0.9921671 ],\n",
       "       [0.00285714, 0.99714286],\n",
       "       [0.00533333, 0.99466667],\n",
       "       [0.00288184, 0.99711816],\n",
       "       [0.99459459, 0.00540541],\n",
       "       [0.38068182, 0.61931818],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05039788, 0.94960212],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0026738 , 0.9973262 ],\n",
       "       [0.99171271, 0.00828729],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05059524, 0.94940476],\n",
       "       [0.10427807, 0.89572193],\n",
       "       [0.08378378, 0.91621622],\n",
       "       [0.0801105 , 0.9198895 ],\n",
       "       [0.00276243, 0.99723757],\n",
       "       [0.01355014, 0.98644986],\n",
       "       [0.95844875, 0.04155125],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04899135, 0.95100865],\n",
       "       [0.99717514, 0.00282486],\n",
       "       [0.0295858 , 0.9704142 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0027027 , 0.9972973 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.47507331, 0.52492669],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99731903, 0.00268097],\n",
       "       [0.08241758, 0.91758242],\n",
       "       [0.00859599, 0.99140401],\n",
       "       [1.        , 0.        ],\n",
       "       [0.11232877, 0.88767123],\n",
       "       [0.99726027, 0.00273973],\n",
       "       [0.0802139 , 0.9197861 ],\n",
       "       [0.04069767, 0.95930233],\n",
       "       [1.        , 0.        ],\n",
       "       [0.83430233, 0.16569767],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9915493 , 0.0084507 ],\n",
       "       [0.24848485, 0.75151515],\n",
       "       [0.02144772, 0.97855228],\n",
       "       [0.00289855, 0.99710145],\n",
       "       [0.75277778, 0.24722222],\n",
       "       [0.01734104, 0.98265896],\n",
       "       [0.03072626, 0.96927374],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00810811, 0.99189189],\n",
       "       [0.99433428, 0.00566572],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94      , 0.06      ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96467391, 0.03532609],\n",
       "       [0.01657459, 0.98342541],\n",
       "       [0.58465608, 0.41534392],\n",
       "       [0.99450549, 0.00549451],\n",
       "       [1.        , 0.        ],\n",
       "       [0.08403361, 0.91596639],\n",
       "       [0.13597734, 0.86402266],\n",
       "       [0.        , 1.        ],\n",
       "       [0.15428571, 0.84571429],\n",
       "       [0.66037736, 0.33962264],\n",
       "       [0.66189112, 0.33810888],\n",
       "       [0.95430108, 0.04569892],\n",
       "       [0.96480938, 0.03519062],\n",
       "       [0.97687861, 0.02312139],\n",
       "       [0.00280899, 0.99719101],\n",
       "       [0.0137741 , 0.9862259 ],\n",
       "       [0.06284153, 0.93715847],\n",
       "       [0.144     , 0.856     ],\n",
       "       [0.92816092, 0.07183908],\n",
       "       [0.00286533, 0.99713467],\n",
       "       [1.        , 0.        ],\n",
       "       [0.07438017, 0.92561983],\n",
       "       [1.        , 0.        ],\n",
       "       [0.64139942, 0.35860058],\n",
       "       [0.23661972, 0.76338028],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00265252, 0.99734748],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0028169 , 0.9971831 ],\n",
       "       [0.07082153, 0.92917847],\n",
       "       [0.34520548, 0.65479452],\n",
       "       [0.01381215, 0.98618785],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99445983, 0.00554017],\n",
       "       [1.        , 0.        ],\n",
       "       [0.06284153, 0.93715847],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0724234 , 0.9275766 ],\n",
       "       [0.0027248 , 0.9972752 ],\n",
       "       [0.06267806, 0.93732194],\n",
       "       [0.90087464, 0.09912536],\n",
       "       [0.        , 1.        ],\n",
       "       [0.93055556, 0.06944444],\n",
       "       [0.        , 1.        ],\n",
       "       [0.31117021, 0.68882979],\n",
       "       [0.        , 1.        ],\n",
       "       [0.87709497, 0.12290503],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00539084, 0.99460916],\n",
       "       [0.89473684, 0.10526316],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00802139, 0.99197861],\n",
       "       [0.79120879, 0.20879121],\n",
       "       [0.00291545, 0.99708455],\n",
       "       [0.00869565, 0.99130435],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02710027, 0.97289973],\n",
       "       [0.09401709, 0.90598291],\n",
       "       [0.02542373, 0.97457627],\n",
       "       [0.01069519, 0.98930481],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0027248 , 0.9972752 ],\n",
       "       [0.98579545, 0.01420455],\n",
       "       [0.        , 1.        ],\n",
       "       [0.08635097, 0.91364903],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00284091, 0.99715909],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99433428, 0.00566572],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.10955056, 0.89044944],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99709302, 0.00290698],\n",
       "       [0.56976744, 0.43023256],\n",
       "       [0.57863501, 0.42136499],\n",
       "       [0.        , 1.        ],\n",
       "       [0.24233983, 0.75766017],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99122807, 0.00877193],\n",
       "       [0.78787879, 0.21212121],\n",
       "       [1.        , 0.        ],\n",
       "       [0.38095238, 0.61904762],\n",
       "       [0.00269542, 0.99730458],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.10354223, 0.89645777],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01072386, 0.98927614],\n",
       "       [0.13105413, 0.86894587],\n",
       "       [0.01971831, 0.98028169],\n",
       "       [0.00543478, 0.99456522],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0056338 , 0.9943662 ]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out of bag score, average of all the scores on the data not included with\n",
    "pipe_clf['baggedDtreeClf'].oob_score_\n",
    "pipe_clf['baggedDtreeClf'].oob_decision_function_ #gives the class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_cap</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_cap  y\n",
       "0        1  1\n",
       "1        1  1\n",
       "2        1  1\n",
       "3        0  0\n",
       "4        0  0\n",
       "..     ... ..\n",
       "109      0  0\n",
       "110      1  1\n",
       "111      0  0\n",
       "112      1  1\n",
       "113      1  1\n",
       "\n",
       "[114 rows x 2 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cap = pipe_clf.predict(X_test)\n",
    "pd.DataFrame({'y_cap': y_cap, 'y':y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.9736842105263158, f1_score: 0.9793103448275862, precision_score: 0.9594594594594594, recall_score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "print('accuracy score: %s, f1_score: %s, precision_score: %s, recall_score: %s' %(accuracy_score(y_test, y_cap),\n",
    "f1_score(y_test, y_cap), precision_score(y_test, y_cap), recall_score(y_test, y_cap)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same model without bagging \n",
    "pipe_clf_unbag = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('decision_clf', DecisionTreeClassifier())\n",
    "])\n",
    "pipe_clf_unbag.fit(X_train, y_train)\n",
    "y_cap = pipe_clf_unbag.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.9298245614035088, f1_score: 0.9428571428571428, precision_score: 0.9565217391304348, recall_score: 0.9295774647887324\n"
     ]
    }
   ],
   "source": [
    "print('accuracy score: %s, f1_score: %s, precision_score: %s, recall_score: %s' %(accuracy_score(y_test, y_cap),\n",
    "f1_score(y_test, y_cap), precision_score(y_test, y_cap), recall_score(y_test, y_cap)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_dev",
   "language": "python",
   "name": "tf_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
