{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ENSEMBLE LEARNING\n",
    "A group of predictors is called an __ensemble__\n",
    "\n",
    "Aggregating the predictions of a group of predictors is __Ensemble Learning__.\n",
    "\n",
    "An Ensemble Learning algorithm is called an __Ensemble method__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifiers\n",
    "\n",
    "<img src=\"../notes_images/ensemble1.png\" width=400 height=350>\n",
    "\n",
    "A very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. This majority-vote classifier is called a hard voting classifier.\n",
    "\n",
    "Voting classifier often achieves a higher accuracy than the best classifier in the ensemble. In fact, even if each classifier is a weak learner (meaning it does only slightly better than random guessing), the ensemble can still be a strong learner (achieving high accuracy), provided there are a sufficient number of weak learners and they are sufficiently diverse.\n",
    "\n",
    "\n",
    "#### Hard Voting Classifier\n",
    "A very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. This majority-vote classifier is called a hard voting classifier\n",
    "\n",
    "#### Soft Voting Classifier\n",
    "If all classifiers are able to estimate class probabilities (i.e., they have a predict_proba() method), then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called soft voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of voting classifier\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
       "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
       "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
       "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
       "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  target  \n",
       "0                            3.92   1065.0       0  \n",
       "1                            3.40   1050.0       0  \n",
       "2                            3.17   1185.0       0  \n",
       "3                            3.45   1480.0       0  \n",
       "4                            2.93    735.0       0  \n",
       "..                            ...      ...     ...  \n",
       "173                          1.74    740.0       2  \n",
       "174                          1.56    750.0       2  \n",
       "175                          1.56    835.0       2  \n",
       "176                          1.62    840.0       2  \n",
       "177                          1.60    560.0       2  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data['data'])\n",
    "df.columns = data['feature_names']\n",
    "df['target'] = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,:len(df.columns)-1],df.iloc[:,len(df.columns)-1], test_size=0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scalar', StandardScaler()),\n",
       "                ('vot_clf',\n",
       "                 VotingClassifier(estimators=[('lc', LogisticRegression()),\n",
       "                                              ('rc', RandomForestClassifier()),\n",
       "                                              ('svc',\n",
       "                                               SVC(C=100, probability=True))],\n",
       "                                  voting='soft'))])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC(kernel='rbf', C=100, probability=True)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "estimators = [('lc',log_clf), ('rc', rnd_clf), ('svc', svm_clf)],\n",
    "    voting= 'soft'#'hard'\n",
    ")\n",
    "\n",
    "pipe_clf = Pipeline(\n",
    "[('scalar', StandardScaler()),('vot_clf',voting_clf)])\n",
    "\n",
    "pipe_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model._logistic.LogisticRegression'> 0.9830508474576272\n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'> 0.9830508474576272\n",
      "<class 'sklearn.svm._classes.SVC'> 0.9830508474576272\n",
      "<class 'sklearn.pipeline.Pipeline'> 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, pipe_clf):\n",
    "    scalar = StandardScaler()\n",
    "    X_train_scaled =  scalar.fit_transform(X_train)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    X_test_scaled = scalar.transform(X_test)\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    print(clf.__class__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVC(kernel='rbf', C=100, probability=True)\n",
    "svm_classifier.fit(X_train,y_train)\n",
    "\n",
    "y_pred = svm_classifier.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01196076, 0.83379765, 0.15424159],\n",
       "       [0.00662472, 0.92475823, 0.06861706],\n",
       "       [0.01787197, 0.71416273, 0.2679653 ],\n",
       "       [0.84252593, 0.12780008, 0.02967399],\n",
       "       [0.40475602, 0.21024496, 0.38499902],\n",
       "       [0.02523919, 0.68669689, 0.28806392],\n",
       "       [0.08594689, 0.33031732, 0.58373579],\n",
       "       [0.00998081, 0.84421051, 0.14580868],\n",
       "       [0.87523394, 0.06930813, 0.05545793],\n",
       "       [0.00524639, 0.93589352, 0.05886009],\n",
       "       [0.74942495, 0.24910006, 0.00147499],\n",
       "       [0.76831637, 0.15446479, 0.07721884],\n",
       "       [0.00695286, 0.87447781, 0.11856933],\n",
       "       [0.82257576, 0.1730821 , 0.00434214],\n",
       "       [0.05830503, 0.38513649, 0.55655848],\n",
       "       [0.00698239, 0.90894507, 0.08407254],\n",
       "       [0.90217358, 0.048742  , 0.04908442],\n",
       "       [0.00504048, 0.92265245, 0.07230707],\n",
       "       [0.12264159, 0.32539103, 0.55196738],\n",
       "       [0.17153853, 0.22585115, 0.60261032],\n",
       "       [0.88757684, 0.09518146, 0.0172417 ],\n",
       "       [0.83675168, 0.12057407, 0.04267425],\n",
       "       [0.07424885, 0.39611485, 0.5296363 ],\n",
       "       [0.91231501, 0.08301438, 0.00467061],\n",
       "       [0.0181309 , 0.73724966, 0.24461945],\n",
       "       [0.83935572, 0.15534191, 0.00530237],\n",
       "       [0.04358248, 0.50282628, 0.45359124],\n",
       "       [0.035592  , 0.56450732, 0.39990067],\n",
       "       [0.87446635, 0.09167197, 0.03386168],\n",
       "       [0.01581013, 0.74694335, 0.23724652],\n",
       "       [0.05250816, 0.47409333, 0.47339851],\n",
       "       [0.15008473, 0.26110377, 0.5888115 ],\n",
       "       [0.84132216, 0.13914851, 0.01952933],\n",
       "       [0.07516931, 0.41184737, 0.51298331],\n",
       "       [0.54739781, 0.15458263, 0.29801956],\n",
       "       [0.02474158, 0.6297446 , 0.34551382],\n",
       "       [0.88678546, 0.10982859, 0.00338595],\n",
       "       [0.58430931, 0.160467  , 0.25522369],\n",
       "       [0.04231056, 0.50407964, 0.4536098 ],\n",
       "       [0.00632518, 0.90375054, 0.08992429],\n",
       "       [0.80173375, 0.10562311, 0.09264315],\n",
       "       [0.0077986 , 0.87927339, 0.11292801],\n",
       "       [0.04141898, 0.48190016, 0.47668085],\n",
       "       [0.00324217, 0.96005572, 0.03670211],\n",
       "       [0.02901912, 0.65338745, 0.31759344],\n",
       "       [0.09608594, 0.28633699, 0.61757707],\n",
       "       [0.84362887, 0.15006009, 0.00631104],\n",
       "       [0.46638219, 0.16263204, 0.37098577],\n",
       "       [0.02802819, 0.64006432, 0.33190749],\n",
       "       [0.04771795, 0.53169989, 0.42058216],\n",
       "       [0.88839841, 0.10766324, 0.00393834],\n",
       "       [0.08071689, 0.34501644, 0.57426667],\n",
       "       [0.02804487, 0.61668295, 0.35527218],\n",
       "       [0.00338187, 0.95720541, 0.03941272],\n",
       "       [0.01617062, 0.74608793, 0.23774145],\n",
       "       [0.1458888 , 0.27477904, 0.57933216],\n",
       "       [0.01668082, 0.74263863, 0.24068054],\n",
       "       [0.86423796, 0.12367349, 0.01208854],\n",
       "       [0.0437938 , 0.51864646, 0.43755974]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'y':pipe_clf.predict(X_test), 'y_true':y_test})\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.50002901e-04 9.98906359e-01 6.43638144e-04]\n",
      " [5.55700469e-04 9.99260716e-01 1.83583303e-04]\n",
      " [1.95797421e-03 5.41542916e-04 9.97500483e-01]\n",
      " [9.21570906e-01 6.55610942e-02 1.28679997e-02]\n",
      " [8.99291499e-01 9.93767934e-02 1.33170810e-03]\n",
      " [3.63437539e-04 9.98499363e-01 1.13719918e-03]\n",
      " [2.38825452e-03 1.38173738e-04 9.97473572e-01]\n",
      " [2.08627583e-04 9.99351280e-01 4.40092573e-04]\n",
      " [4.66097296e-01 5.33198557e-01 7.04146920e-04]\n",
      " [2.90825716e-03 9.96917407e-01 1.74335788e-04]\n",
      " [9.99961516e-01 4.73818703e-06 3.37456057e-05]\n",
      " [9.14234330e-01 7.41631603e-02 1.16025098e-02]\n",
      " [2.89766066e-04 2.97111083e-03 9.96739123e-01]\n",
      " [9.97794175e-01 1.47804298e-03 7.27781766e-04]\n",
      " [3.07784620e-03 4.52739233e-04 9.96469415e-01]\n",
      " [6.32474347e-03 9.85906907e-01 7.76834969e-03]\n",
      " [5.35898389e-01 4.63634986e-01 4.66625595e-04]\n",
      " [1.77274719e-02 9.81981476e-01 2.91052625e-04]\n",
      " [1.27284167e-01 8.67725178e-01 4.99065517e-03]\n",
      " [3.48211211e-03 2.31106289e-04 9.96286782e-01]\n",
      " [9.94439486e-01 2.99971613e-03 2.56079800e-03]\n",
      " [9.99479495e-01 4.40573556e-04 7.99318103e-05]\n",
      " [5.73282773e-03 7.17920754e-02 9.22475097e-01]\n",
      " [9.97513975e-01 1.43610050e-03 1.04992452e-03]\n",
      " [2.12887698e-04 9.99678159e-01 1.08953781e-04]\n",
      " [9.45094808e-01 4.72871968e-02 7.61799480e-03]\n",
      " [5.70824654e-04 4.25328993e-02 9.56896276e-01]\n",
      " [3.60880769e-04 9.99420412e-01 2.18706845e-04]\n",
      " [9.95227643e-01 3.16557107e-03 1.60678556e-03]\n",
      " [7.39294916e-04 9.88958404e-01 1.03023008e-02]\n",
      " [5.52145360e-04 9.99058193e-01 3.89662101e-04]\n",
      " [9.41403252e-04 1.03596153e-03 9.98022635e-01]\n",
      " [9.95494943e-01 4.38399882e-03 1.21058509e-04]\n",
      " [3.26803156e-02 9.36426156e-01 3.08935282e-02]\n",
      " [9.97791885e-01 1.43587737e-03 7.72237340e-04]\n",
      " [5.97703714e-04 1.16196750e-02 9.87782621e-01]\n",
      " [9.99288010e-01 1.65946211e-04 5.46043538e-04]\n",
      " [8.54321645e-01 1.43934838e-01 1.74351682e-03]\n",
      " [4.61586271e-03 4.54141445e-02 9.49969993e-01]\n",
      " [3.43084744e-03 9.96046612e-01 5.22540461e-04]\n",
      " [9.98516653e-01 9.87847567e-04 4.95499019e-04]\n",
      " [7.49524285e-05 9.99761167e-01 1.63880553e-04]\n",
      " [6.78921871e-04 8.03751885e-05 9.99240703e-01]\n",
      " [4.64885451e-04 9.98116545e-01 1.41856969e-03]\n",
      " [2.42469527e-02 9.72965560e-01 2.78748766e-03]\n",
      " [8.61827053e-03 9.15881272e-05 9.91290141e-01]\n",
      " [9.99276841e-01 6.11326238e-04 1.11832405e-04]\n",
      " [4.36712110e-02 9.56162358e-01 1.66430805e-04]\n",
      " [9.97696972e-04 7.12698214e-02 9.27732482e-01]\n",
      " [1.48078848e-03 6.02187360e-04 9.97917024e-01]\n",
      " [9.98610858e-01 1.78431470e-04 1.21071030e-03]\n",
      " [1.05634292e-03 1.11485772e-03 9.97828799e-01]\n",
      " [8.80585597e-04 4.40180557e-02 9.55101359e-01]\n",
      " [1.10601511e-03 9.98197833e-01 6.96152261e-04]\n",
      " [1.28136352e-03 9.87380595e-01 1.13380416e-02]\n",
      " [2.40583899e-02 1.06674791e-02 9.65274131e-01]\n",
      " [4.49422537e-02 9.54986631e-01 7.11150768e-05]\n",
      " [9.97200735e-01 2.39675955e-03 4.02505819e-04]\n",
      " [3.13320764e-04 9.99454942e-01 2.31737475e-04]]\n",
      "[[0.04 0.92 0.04]\n",
      " [0.01 0.98 0.01]\n",
      " [0.01 0.02 0.97]\n",
      " [0.88 0.08 0.04]\n",
      " [0.64 0.33 0.03]\n",
      " [0.03 0.66 0.31]\n",
      " [0.   0.   1.  ]\n",
      " [0.   1.   0.  ]\n",
      " [0.73 0.27 0.  ]\n",
      " [0.02 0.98 0.  ]\n",
      " [0.99 0.01 0.  ]\n",
      " [0.81 0.14 0.05]\n",
      " [0.   0.11 0.89]\n",
      " [0.89 0.11 0.  ]\n",
      " [0.14 0.29 0.57]\n",
      " [0.02 0.96 0.02]\n",
      " [0.3  0.7  0.  ]\n",
      " [0.   0.97 0.03]\n",
      " [0.17 0.8  0.03]\n",
      " [0.07 0.14 0.79]\n",
      " [0.96 0.02 0.02]\n",
      " [1.   0.   0.  ]\n",
      " [0.   0.29 0.71]\n",
      " [0.94 0.06 0.  ]\n",
      " [0.   1.   0.  ]\n",
      " [0.86 0.14 0.  ]\n",
      " [0.   0.15 0.85]\n",
      " [0.01 0.99 0.  ]\n",
      " [0.99 0.01 0.  ]\n",
      " [0.   0.94 0.06]\n",
      " [0.   0.99 0.01]\n",
      " [0.   0.   1.  ]\n",
      " [0.89 0.11 0.  ]\n",
      " [0.06 0.8  0.14]\n",
      " [0.98 0.01 0.01]\n",
      " [0.01 0.2  0.79]\n",
      " [0.99 0.01 0.  ]\n",
      " [0.92 0.08 0.  ]\n",
      " [0.   0.12 0.88]\n",
      " [0.13 0.75 0.12]\n",
      " [1.   0.   0.  ]\n",
      " [0.   1.   0.  ]\n",
      " [0.   0.06 0.94]\n",
      " [0.   0.99 0.01]\n",
      " [0.02 0.95 0.03]\n",
      " [0.16 0.28 0.56]\n",
      " [0.96 0.04 0.  ]\n",
      " [0.04 0.93 0.03]\n",
      " [0.   0.11 0.89]\n",
      " [0.04 0.22 0.74]\n",
      " [1.   0.   0.  ]\n",
      " [0.   0.   1.  ]\n",
      " [0.   0.21 0.79]\n",
      " [0.   0.99 0.01]\n",
      " [0.01 0.95 0.04]\n",
      " [0.04 0.11 0.85]\n",
      " [0.29 0.67 0.04]\n",
      " [0.99 0.01 0.  ]\n",
      " [0.01 0.98 0.01]]\n",
      "[[5.30307505e-03 9.90596565e-01 4.10035995e-03]\n",
      " [1.78397394e-03 9.95663419e-01 2.55260691e-03]\n",
      " [8.76333735e-03 2.65918642e-02 9.64644798e-01]\n",
      " [9.63959993e-01 1.77963309e-02 1.82436762e-02]\n",
      " [9.19249862e-01 7.22211160e-02 8.52902217e-03]\n",
      " [3.71229900e-02 9.24156732e-01 3.87202782e-02]\n",
      " [9.99352868e-03 1.09374668e-02 9.79069005e-01]\n",
      " [4.66268760e-03 9.77070041e-01 1.82672714e-02]\n",
      " [2.02314627e-01 7.37088392e-01 6.05969806e-02]\n",
      " [5.59660229e-03 9.81932401e-01 1.24709971e-02]\n",
      " [9.43139964e-01 3.41886353e-02 2.26714007e-02]\n",
      " [8.91173662e-01 8.72430937e-02 2.15832439e-02]\n",
      " [2.13664861e-02 8.74282433e-02 8.91205271e-01]\n",
      " [9.91286337e-01 2.49141711e-03 6.22224541e-03]\n",
      " [6.21417154e-02 2.27316062e-01 7.10542223e-01]\n",
      " [3.13410724e-02 9.26909347e-01 4.17495804e-02]\n",
      " [4.19787855e-02 9.00024976e-01 5.79962387e-02]\n",
      " [1.23883491e-02 9.67629010e-01 1.99826413e-02]\n",
      " [1.53844481e-01 8.31391481e-01 1.47640374e-02]\n",
      " [1.11614249e-02 2.65276924e-02 9.62310883e-01]\n",
      " [9.80880557e-01 8.38141912e-03 1.07380234e-02]\n",
      " [9.93617093e-01 3.36501258e-03 3.01789394e-03]\n",
      " [9.57396260e-03 3.58969511e-02 9.54529086e-01]\n",
      " [9.03022202e-01 6.67741662e-02 3.02036319e-02]\n",
      " [2.34350368e-05 9.96889391e-01 3.08717375e-03]\n",
      " [9.61828616e-01 1.74878819e-02 2.06835017e-02]\n",
      " [5.32069526e-03 4.68000060e-02 9.47879299e-01]\n",
      " [6.70274269e-03 9.78881802e-01 1.44154554e-02]\n",
      " [9.87140076e-01 3.64428395e-03 9.21563966e-03]\n",
      " [2.71084501e-03 9.84601267e-01 1.26878876e-02]\n",
      " [3.48630893e-03 9.86042381e-01 1.04713098e-02]\n",
      " [1.07874585e-02 6.57152827e-02 9.23497259e-01]\n",
      " [7.54750342e-01 2.14073102e-01 3.11765558e-02]\n",
      " [8.29346312e-02 8.42538544e-01 7.45268248e-02]\n",
      " [9.92800278e-01 2.13262103e-03 5.06710102e-03]\n",
      " [5.46821541e-03 2.26970983e-02 9.71834686e-01]\n",
      " [9.85458378e-01 2.81776100e-03 1.17238613e-02]\n",
      " [9.43884891e-01 5.03950752e-02 5.72003362e-03]\n",
      " [5.97392570e-03 7.55426593e-02 9.18483415e-01]\n",
      " [6.89633917e-03 9.84116334e-01 8.98732725e-03]\n",
      " [9.89359679e-01 5.31639898e-03 5.32392171e-03]\n",
      " [2.74696913e-05 9.96236695e-01 3.73583505e-03]\n",
      " [1.36738374e-02 3.74531992e-02 9.48872963e-01]\n",
      " [5.00551895e-03 9.89727196e-01 5.26728537e-03]\n",
      " [1.34321178e-02 9.65913049e-01 2.06548334e-02]\n",
      " [9.17063178e-02 5.74223054e-01 3.34070628e-01]\n",
      " [9.86436066e-01 6.48665493e-03 7.07727907e-03]\n",
      " [3.01977110e-02 9.13639230e-01 5.61630593e-02]\n",
      " [4.81055131e-03 5.44506746e-02 9.40738774e-01]\n",
      " [1.13656554e-02 3.42484494e-02 9.54385895e-01]\n",
      " [9.82184643e-01 5.99650669e-03 1.18188500e-02]\n",
      " [2.99772769e-03 4.24097151e-03 9.92761301e-01]\n",
      " [1.82320088e-02 1.58553378e-01 8.23214613e-01]\n",
      " [2.13496511e-03 9.91401017e-01 6.46401834e-03]\n",
      " [1.60860804e-03 9.93312724e-01 5.07866784e-03]\n",
      " [7.65860874e-03 1.24224691e-02 9.79918922e-01]\n",
      " [2.62821245e-02 9.46349414e-01 2.73684614e-02]\n",
      " [9.78086220e-01 1.54559648e-02 6.45781469e-03]\n",
      " [2.92049002e-03 9.85857713e-01 1.12217974e-02]]\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC(kernel='rbf', C=100, probability=True)\n",
    "\n",
    "std_scalar = StandardScaler()\n",
    "X_train_scaled = std_scalar.fit_transform(X_train)\n",
    "log_clf.fit(X_train_scaled,y_train)\n",
    "rnd_clf.fit(X_train_scaled,y_train)\n",
    "svm_clf.fit(X_train_scaled,y_train)\n",
    "\n",
    "X_test_scaled = std_scalar.transform(X_test)\n",
    "\n",
    "print(log_clf.predict_proba(X_test_scaled))\n",
    "print(rnd_clf.predict_proba(X_test_scaled))\n",
    "print(svm_clf.predict_proba(X_test_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAGGING AND PASTING\n",
    "Using the same training algorithm for every predictor(e.g. regressor), but to train them on different random subsets of the training set. \n",
    "When sampling is performed \n",
    "1. with replacement, this method is called bagging(short forbootstrap aggregating \n",
    "2. without replacement, it is called pasting.\n",
    "\n",
    "__Bagging__ takes the model with high variance and low bias and reduces the variance without affecting the bias.\n",
    "\n",
    "__Boosting__ takes the model with low variance and high bias and reduces the bias without affecting the variance.\n",
    "\n",
    "Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. \n",
    "\n",
    "The aggregation function  typically :\n",
    "1. The statistical mode (i.e., the most frequent prediction, just like a hard voting classifier) for classification\n",
    "2. The average for regression.\n",
    "\n",
    "Each individual predictor has a higher bias than if it were trained on the original training set, but\n",
    "aggregation reduces both bias and variance.\n",
    "\n",
    "Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set.\n",
    "\n",
    "All predictors can run on different core therefore they are faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04575308, 2.90950292, 0.044744  ])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_dev",
   "language": "python",
   "name": "tf_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
