{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOOSTING\n",
    "\n",
    "Boosting (originally called hypothesis boosting) refers to any Ensemble method that can combine several weak learners into a strong learner. \n",
    "\n",
    "The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor.Popular boosting algorithms:\n",
    "\n",
    "a. AdaBoost(Adaptive Boosting)\n",
    "\n",
    "b. Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "By paying more attention to the training instances that were __underfitted__ by the preceeding predictors.\n",
    "This results in the new predictor focussing more and more on the hard cases. This is the technique used in the case of  adaptive boosting.\n",
    "\n",
    "###### Procedure\n",
    "1. A base classifier is trained to make the predictions on the training set.\n",
    "2. The relative weight of misclassified training example is then increased and a second classifier is then used on the new dataset with updated weights.\n",
    "3. This process keeps on repeating until\n",
    "4. This appears like the batch gradient descent, but instead of tweaking single predictor parameter to minimize the cost function AdaBoost adds predictors to the ensemble.\n",
    "5. After all the predictors are trained, it predicts by voting and averaging/max mode like bagging.\n",
    "\n",
    "Technique cannot be parallelized hence it does not scales well.\n",
    "\n",
    "\n",
    "Calculation for AdaBoost\n",
    "\n",
    "1. The weight is initially set to $\\frac{1}{m}$\n",
    "2. __Weighted error rate__ for $j^{th}$ predictor:\n",
    "$$r_{j} = \\frac{\\sum_{i=1, \\hat{y}_{j}^{i}\\neq y^{(i)}}^{m} w^{(i)}}{\\sum_{i=1}^{m}w^{(i)}}$$\n",
    "3. Then __predictor's weight__ $\\alpha_{j}$ is calculated, where $\\eta$ is the learning rate(defaulting to 1). The acccurate the pridictor is, higher its weight will be.\n",
    "$$\\alpha = \\eta log\\frac{1-r_{j}}{r_{j}}$$\n",
    "\n",
    "The equation for boosting instance weights:\n",
    "\n",
    "for i= 1,2,3....,m\n",
    "\n",
    "$$w^{(i)}= \n",
    "\\begin{cases}\n",
    "    w^{(i)} (\\text{or sometimes} w^{(i)} \\ e^{(a_{-j})}),& \\text{if } \\hat{y}^{(i)}= y^{i}\\\\\n",
    "    w^{(i)} \\ e^{(a_{j})},& \\text{if } \\hat{y}^{(i)}\\neq y^{i}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "4. Then all the weights are normalized i.e divided by $\\sum_{i=1}{m} w^{(i)}$\n",
    "\n",
    "5. A new predictor is trained using the updated weights\n",
    "6. The whole process is repeated (the new predictor’s weight is computed, the instance weights are updated, then another predictor is trained, and so on). The algorithm stops when the desired number of predictors is reached, or when a perfect predictor is found.\n",
    "7. AdaBoost simply computes the predictions of all the predictors and weighs them using the predictor weights $\\alpha_{j}$ . The predicted class is the one that receives the majority of weighted votes.\n",
    "\n",
    "$\\hat{y}(x) = argmax_{k} \\sum_{j=1, \\hat{y}_{j}(x) = k}^{N} \\alpha_{j}$\n",
    "8. If using the decision tree and stumps in AdaBoost, we chose the feature for making stumps in the same way as the Decision Tree by checking and comparing the gini index(purity) index of all the stumps.\n",
    "\n",
    "If your AdaBoost ensemble is overfitting the training set, you can try reducing the number of estimators or more strongly regularizing the base estimator.\n",
    "\n",
    "STUMPS - It is a parent node and two leafs. They can only use 1 variable to make a decision. Hence they are weak learners.\n",
    "\n",
    "\n",
    "Difference between AdaBoost and Random Forest:\n",
    "\n",
    "|Sno.|Random Forest|AdaBoost|\n",
    "|----|-------------|--------|\n",
    "|1   |It uses the entire tree to make the decision|It just uses stumps(weak learners) to make decisions|\n",
    "|2   |All the trees in random forest contribute equally towards the votes|In AdaBoost the vote of the stumps is weighted|\n",
    "|3   |All the trees are made independantly in Random Forest|In AdaBoost order of stumps in the forest is important|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                         class_weight=None,\n",
       "                                                         criterion='gini',\n",
       "                                                         max_depth=1,\n",
       "                                                         max_features=None,\n",
       "                                                         max_leaf_nodes=None,\n",
       "                                                         min_impurity_decrease=0.0,\n",
       "                                                         min_impurity_split=None,\n",
       "                                                         min_samples_leaf=1,\n",
       "                                                         min_samples_split=2,\n",
       "                                                         min_weight_fraction_leaf=0.0,\n",
       "                                                         presort='deprecated',\n",
       "                                                         random_state=None,\n",
       "                                                         splitter='best'),\n",
       "                   learning_rate=0.5, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], test_size=.20)\n",
    "ada_clf = AdaBoostClassifier(\n",
    "DecisionTreeClassifier(max_depth=1), \n",
    "    n_estimators=200, \n",
    "    algorithm=\"SAMME.R\", \n",
    "    learning_rate=0.5\n",
    ")\n",
    "\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.9855072463768116, recall:1.0, f1:0.9927007299270074, accuracy_score:0.9912280701754386\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "y_hat = ada_clf.predict(X_test)\n",
    "print('precision: %s, recall:%s, f1:%s, accuracy_score:%s' %(precision_score(y_test, y_hat),\n",
    "                                                            recall_score(y_test, y_hat),\n",
    "                                                            f1_score(y_test, y_hat),\n",
    "                                                            accuracy_score(y_test, y_hat)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$precision = \\frac{T.P.}{T.P. + F.P.}$$\n",
    "$$recall = \\frac{T.P.}{F.N. + T.P.}$$\n",
    "$$recall = \\frac{1}{\\frac{1}{precision}+\\frac{1}{recall}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRADIENT BOOSTING\n",
    "Gradient Boosting. 17 Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor.\n",
    "\n",
    "#### GRADIENT BOOSTED REGRESSION TREES (GBRT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import load_diabetes\n",
    "import pandas as pd\n",
    "data = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], test_size=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X_train, y_train)\n",
    "y1 = tree_reg1.predict(X_train)\n",
    "\n",
    "y2 = y_train-y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2 = tree_reg2.fit(X_train, y2)\n",
    "y11 = tree_reg2.predict(X_train)\n",
    "y3 = y2 - y11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3 = tree_reg3.fit(X_train, y3)\n",
    "# y111 = tree_reg3.predict(X_train)\n",
    "# y = y2 - y111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = sum([tree.predict(X_test) for tree in (tree_reg1, tree_reg2, tree_reg3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean square error: 48.669468976897456\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "print('mean absolute error: %s' % (mean_absolute_error(y_hat, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean absolute error: 51.42778548530094\n"
     ]
    }
   ],
   "source": [
    "y_hat2 = tree_reg1.predict(X_test)\n",
    "print('mean absolute error: %s' % (mean_absolute_error(y_test, y_hat2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1)\n",
    "gbrt.fit(X_train,y_train)\n",
    "y_hat = gbrt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean absolute error : 48.66946897689745\n"
     ]
    }
   ],
   "source": [
    "print('mean absolute error : %s' %(mean_absolute_error(y_hat, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
       "                          init=None, learning_rate=0.1, loss='ls', max_depth=2,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=120,\n",
       "                          n_iter_no_change=None, presort='deprecated',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = [mean_absolute_error(y_test, y_pred) \n",
    "          for y_pred in gbrt.staged_predict(X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = np.argmin(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
       "                          init=None, learning_rate=0.1, loss='ls', max_depth=2,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=52,\n",
       "                          n_iter_no_change=None, presort='deprecated',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=best_estimator)\n",
    "gbrt_best.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "early stoppage can be implemented as follows:\n",
    "```python\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "gbrt.n_estimators = n_estimators\n",
    "gbrt.fit(X_train, y_train)\n",
    "y_pred = gbrt.predict(X_val)\n",
    "val_error = mean_squared_error(y_val, y_pred)\n",
    "if val_error < min_val_error:\n",
    "min_val_error = val_error\n",
    "error_going_up = 0\n",
    "else:\n",
    "error_going_up += 1\n",
    "if error_going_up == 5:\n",
    "break # early stopping\n",
    "```\n",
    "\n",
    "XGBoost is an optimized implementation of gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STACKING\n",
    "It is based on a simple idea: instead of using trivial functions (such as hard voting) to aggregate the predictions of all predictors in an ensemble, why don’t we train a model to perform this aggregation.\n",
    "\n",
    "<img src =\"../notes_images/stacking1.png\" height=300 width=400>\n",
    "\n",
    "\n",
    "\n",
    "To train the blender, a common approach is to use a hold-out set.First, the training set is split in two subsets. The first subset is used to train the\n",
    "predictors in the first layer:\n",
    "\n",
    "<img src =\"../notes_images/stacking2.png\" height=300 width=400>\n",
    "\n",
    "Next, the first layer predictors are used to make predictions on the second (held-out)\n",
    "set. This ensures that the predictions are “clean,” since the predictors never saw these instances during training. <img src =\"../notes_images/stacking3.png\" height=300 width=400>\n",
    "\n",
    "Now for each instance in the hold-out set there are three predicted values. We can create a new training set using these predicted values as input features (which makes this new training set three-dimensional), and keeping the target values. The blender is trained on this new training set, so it learns to predict the target value given the first layer’s predictions.\n",
    "\n",
    "\n",
    "It is actually possible to train several different blenders this way (e.g., one using Linear Regression, another using Random Forest Regression, and so on): we get a whole layer of blenders. The trick is to split the training set into three subsets: the first one is used to train the first layer, the second one is used to create the training set used to train the second layer (using predictions made by the predictors of the first layer), and the third one is used to create the training set to train the third layer (using predictions made by the predictors of the second layer). Once this is done, we can make a prediction for a new instance by going through each layer sequentially as follows:\n",
    "\n",
    "<img src =\"../notes_images/stacking4.png\" height=300 width=400>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_env",
   "language": "python",
   "name": "image_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
