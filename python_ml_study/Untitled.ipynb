{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction\n",
    "Highdimensional datasets are at risk of being very sparse: most training instances are likely to be far away from each other. Of course, this also means that a new instance will likely be far away from any training instance, making predictions much less reliable than in lower dimensions, since they will be based on much larger  extrapolations.\n",
    "\n",
    "Reasons for dimensionlity reduction:\n",
    "1. Data Compression\n",
    "2. Speeding up the learining process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approaches For Dimension Reduction\n",
    "\n",
    "#### Projection\n",
    "In most real-world problems, training instances are not spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated. All training instances actually lie within (or close to) a much lower-dimensional subspace of the high-dimensional space. \n",
    "If we project every training instance perpendicularly onto this subspace (as represented by the short lines con‐\n",
    "necting the instances to the plane), we get the new 2D dataset. The two axis of new 2-D plan will be different from the original plane.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"../notes_images/dimen_red_projection1.png\" width=300 height=350></td>\n",
    "        <td><img src=\"../notes_images/dimen_red_projection2.png\" width=300 height=350></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3D projection close to 2D subspace</td>\n",
    "        <td>Projected on 2D subspaces</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "However, projection is not always the best approach to dimensionality reduction. In many cases the subspace may twist and turn, such as in the famous Swiss roll toy dataset.\n",
    "Simply projecting onto a plane (e.g., by dropping x 3 ) would squash different layers of the Swiss roll together.\n",
    "\n",
    "<img src=\"../notes_images/swiss_role_ds.png\" width=300 height=350>\n",
    "swiss role dataset \n",
    "<img src=\"../notes_images/squished_swiss_role.png\" width=500 height=350>\n",
    "Squished swiss role\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold Learning\n",
    "\n",
    "A 2D manifold is a 2D shape that can be bent and twisted in a higher-dimensional space. More generally, a\n",
    "d-dimensional manifold is a part of an n-dimensional space (where d < n) that locally resembles a d-dimensional hyperplane.\n",
    "\n",
    "In the case of the Swiss roll, d = 2 and n = 3: it locally resembles a 2D plane, but it is rolled in the third dimension.\n",
    "\n",
    "Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie; this is called Manifold Learning. It relies on the manifold assumption, also called the manifold hypothesis, which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold. This\n",
    "assumption is very often empirically observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "Principal Component Analysis (PCA) is by far the most popular dimensionality reduction algorithm. First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it.\n",
    "\n",
    "The PCA tries to calculate a projection plane, so as to minimize the projction error of all the data points.\n",
    "\n",
    "** Common practise is to calculate mean and standardize the dataset.\n",
    "\n",
    "In general it first finds the `k` vectors $u^{(1)},u^{(2)},u^{(3)},...u^{(k)}$ from n vectors, onto which to project the data so as to minimize the projection error.\n",
    "\n",
    "These `k` vectors are the vectors that __preserve the most variance__, as it will most likely lose less information than the other projections. \n",
    "\n",
    "Another way to justify this choice is that it is the axis that __minimizes the mean squared distance__ between the original dataset and its projection onto that axis.\n",
    "\n",
    "PCA identifies the axis that accounts for the largest amount of variance in the training set. \n",
    "\n",
    "It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of remaining variance. \n",
    "\n",
    "If it were a higher-dimensional dataset, PCA would also find a third axis, orthogonal to both previous axes, and a fourth and fifth, and so on—as many axes as the number of dimensions in the dataset.\n",
    "\n",
    "There exists a technique called SVD(Single Value Decomposition) that factorises(or decomposes) the matrix `X` into the multiplication of three vectors $U\\sum V^T$. Where V contains all the PCs(Principle Components)\n",
    "\n",
    "Procedure to calculate Principle Component Analysis algorithm:\n",
    "\n",
    "1. Reduce data from n-dimensions to k-dimensions.\n",
    "2. Compute `covariance matrix`\n",
    "\n",
    "$$\\sum = \\frac{1}{m}\\sum_{i=1}^{n} (x^{(i)})(x^{(i)})^{T}$$\n",
    "\n",
    "3. Compute eigenvectors of matrix $\\sum$:\n",
    "\n",
    "    [u,s,v] = svd(Sigma)  ,where svd = singular value decomposition\n",
    "\n",
    "#### Steps to calculate PCA:\n",
    "1. Standardize the data.\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "wine_ds = load_wine()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine_ds['data'], wine_ds['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Standardization\n",
    "std_sca = StandardScaler()\n",
    "X_train_norm = std_sca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance Matrix\n",
    "$$\\sum = \\frac{\\big((X - \\bar{x})^T (X - \\bar{x})\\big)}{n-1}$$\n",
    "\n",
    "where, $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i$\n",
    "\n",
    "### EigenVectors vs EigenValues \n",
    "The vectors that are not knocked off from their basis vector before/after the transformations they are merely squished or streched.\n",
    "\n",
    "If roation operation is performed on a 3D space, then if eigen value remains unchanged than its the axis of that rotation.\n",
    "\n",
    "$$A\\vec{v} = \\lambda \\vec{v}$$\n",
    "$$det(\\vec{v}(A  - I\\lambda)) = 0$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vec = np.mean(X_train_norm,axis=0)\n",
    "cov_mat = (X_train_norm - mean_vec).T.dot((X_train_norm - mean_vec))/(X_train_norm.shape[0]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_val, eig_vecs = np.linalg.eig(cov_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix\n",
    "Especially, in the field of “Finance,” the correlation matrix typically used instead of the covariance matrix. However, the eigendecomposition of the covariance matrix (if the input data was standardized) yields the same results as a eigendecomposition on the correlation matrix, since the correlation matrix can be understood as the normalized covariance matrix.\n",
    "This yields the sa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_dev",
   "language": "python",
   "name": "tf_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
