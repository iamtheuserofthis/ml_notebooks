{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties\n",
    "1. Lexical Attributes: They don't depend on the attributes of the words. Refers words from the vocab(String store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: [0, 1, 2, 3]\n",
      "Text: ['It', 'costs', '$', '5']\n",
      "is_alpha: [True, True, False, False]\n",
      "is_punct: [False, False, False, False]\n",
      "like_num: [False, False, False, True]\n",
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(\"It costs $5\")\n",
    "\n",
    "print(\"Index:\",[token.i for token in doc])\n",
    "print(\"Text:\",[token.text for token in doc])\n",
    "print(\"is_alpha:\",[token.is_alpha for token in doc])\n",
    "print(\"is_punct:\",[token.is_punct for token in doc])\n",
    "print(\"like_num:\",[token.like_num for token in doc])\n",
    "\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It PRON nsubj costs\n",
      "costs VERB ROOT costs\n",
      "$ SYM nmod 5\n",
      "5 NUM dobj costs\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$5 MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labelled data that the model was trained on is __NOT__ included in spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9528407286733565721, 5, 7)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"TEXT\":\"iPhone\"}, {\"TEXT\":\"X\"}]\n",
    "matcher.add(\"IPHONE_PATTERN\",[pattern])\n",
    "doc  = nlp(\"the phone I found was iPhone X yesterday\")\n",
    "matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.5\n",
      "python 3.7\n",
      "python 3\n",
      "python 2\n"
     ]
    }
   ],
   "source": [
    "string = \"\"\"\n",
    "python 3.5 and python 3.7 onwards have the difference that the former is not async.\n",
    "python 3 and python 2 are very different in itself.\n",
    "\"\"\"\n",
    "doc3 = nlp(string)\n",
    "pattern = [{\"TEXT\":\"python\"}, {\"LIKE_NUM\":True}]\n",
    "\n",
    "matcher2 = Matcher(nlp.vocab)\n",
    "matcher2.add(\"pyt_pattern\",[pattern])\n",
    "\n",
    "for hashv, start, end in matcher2(doc3):\n",
    "    print(doc3[start:end])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|OP\t|Description|\n",
    "|---|-----------|\n",
    "|!\t|Negate the pattern, by requiring it to match exactly 0 times.|\n",
    "|?\t|Make the pattern optional, by allowing it to match 0 or 1 times.|\n",
    "|+\t|Require the pattern to match 1 or more times.|\n",
    "|*\t|Allow the pattern to match 0 or more times.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a doc manually\n",
    "from spacy.tokens import Doc,Span\n",
    "\n",
    "words = [\"This\", \"is\", \"it\", \"Carlos\"]\n",
    "spaces = [True, True, True, False]\n",
    "doc= Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "span = Span(doc, 0,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer cases\n",
    "\n",
    "The tokenizer doesn’t create tokens for single spaces, so there’s no token with the value \" \" in between.\n",
    "\n",
    "\n",
    "Sometimes it’s more efficient to match exact strings instead of writing patterns describing the individual tokens. This is especially true for finite categories of things – like all countries of the world. We already have a list of countries, so let’s use this as the basis of our information extraction script. A list of string names is available as the variable COUNTRIES.\n",
    "\n",
    "    Import the PhraseMatcher and initialize it with the shared vocab as the variable matcher.\n",
    "    Add the phrase patterns and call the matcher on the doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_V_PAT\n",
      "silicon Valley\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "pattern1 = [{\"LOWER\": \"silicon\"}, {\"TEXT\": \" \"}, {\"LOWER\":\"Valley\"}]\n",
    "pattern2 = [{\"LOWER\": \"silicon\"}, {\"LOWER\":\"valley\"}]\n",
    "doc111 = nlp(\"People in the silicon Valley etc\")\n",
    "matcher11 = Matcher(nlp.vocab)\n",
    "matcher11.add(\"S_V_PAT\",[pattern2])\n",
    "\n",
    "for hsh,start,end in matcher11(doc111):\n",
    "    print(doc111.vocab.strings[hsh])\n",
    "    print(doc111[start:end])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{\"LOWER\": \"amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "pattern2 = [{\"LOWER\": \"ad\"},{\"IS_PUNCT\":True},{\"LOWER\":\"free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", [pattern1])\n",
    "matcher.add(\"PATTERN2\", [pattern2])\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "# checking the names in the nlp pipeline components\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom NLP Pipelines\n",
    "\n",
    "1. Cand add custom components to the pipeline\n",
    "2. The function takes doc and returns doc can be a custom function.\n",
    "3. They are useful to \n",
    "    \n",
    "    a. Calculate custom value based on the token and their attribute.\n",
    "\n",
    "    b. Added named entitiy based on a dictionary.\n",
    "3. ```python\n",
    "nlp.add_pipe(component, \n",
    "last/first/before=bool/bool/'comp')\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "['tok2vec', 'tagger', 'parser', 'ner', 'animal_detect', 'attribute_ruler', 'lemmatizer']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "## e.g. of a custom entities in pipeline, rule based\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", None, *animal_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"animal_detect\")\n",
    "def animal_component(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=match_id) for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "#instead of annotation also: Language.component(\"my_component2\", func=my_component)\n",
    "# Add the component to the pipeline after the \"ner\" component\n",
    "# depricated ----\n",
    "nlp.add_pipe(\"animal_detect\",after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accessible with `._ property`\n",
    "\n",
    "### Types of extensions attributes:\n",
    "1. Attribute extensions\n",
    "   :Sets a default value that can be overwritten.\n",
    "2. Property extensions\n",
    "   : Behaves like properties is python, has both getter and setter.\n",
    "3. Method extensions: Makes the attributes a method that can be calculated dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "#force=True to reset extension\n",
    "Token.set_extension(\"is_color\", default=False, force=True)\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "doc[3]._.is_color=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property extension\n",
    "from spacy.tokens import Span\n",
    "\n",
    "def get_has_color(token):\n",
    "    colors = [\"red\",\"yellow\",\"blue\"]\n",
    "    return token.text in colors\n",
    "\n",
    "Token.set_extension(\"has_color\", getter=get_has_color, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"this is a blue dog\")\n",
    "\n",
    "doc[3]._.has_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_.has_color True False\n"
     ]
    }
   ],
   "source": [
    "# property extension for span\n",
    "\n",
    "def has_color(span):\n",
    "    colors = [\"red\",\"yellow\",\"blue\"]\n",
    "    return any([token.text in colors for token in span])\n",
    "Span.set_extension(\"has_al_colors\", getter=has_color, force=True)\n",
    "\n",
    "doc = nlp(\"the colur of our sky is not blue\")\n",
    "print(\"_.has_color\", doc[1:8]._.has_color, doc[1:5]._.has_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "def has_token(doc:Doc, token_text:str)-> bool:\n",
    "    has_tok = token_text in [token.text for token in doc]\n",
    "    return has_tok\n",
    "\n",
    "Doc.set_extension(\"has_token\", method=has_token, force=True)\n",
    "\n",
    "doc= nlp(\"My dog did go crazy when he saw the color blue\")\n",
    "\n",
    "doc._.has_token(\"did\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating custom components and entities\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "\n",
    "# Set the Span extension wikipedia_url using the getter get_wikipedia_url\n",
    "Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\n",
    "    \"In over fifty years from his very first recordings right through to his \"\n",
    "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_env",
   "language": "python",
   "name": "image_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
