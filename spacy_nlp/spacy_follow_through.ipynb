{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistc Terms:\n",
    "1. Adposition(ADP):\n",
    "    \n",
    "    An adposition is a cover term for prepositions and postpositions. It is a member of a closed set of items that:\n",
    "    a. Occur before or after a complement composed of a noun phrase, noun, pronoun, or clause that functions as a noun phrase.\n",
    "    b. Form a single structure with the complement to express its grammatical and semantic relation to another unit within a clause.\n",
    "    \n",
    "2. A determiner is a word or affix that belongs to a class of noun modifiers that expresses the reference, including quantity, of a noun.\n",
    "\n",
    "\n",
    "3. Complement\n",
    "\n",
    "    Traditionally, a complement is a constituent of a clause, such as a noun phrase or adjective phrase, that is used to predicate a description of the subject or object of the clause.\n",
    "\n",
    "2. Nominal subject(nsubj): A nominal subject ( nsubj ) is a nominal which is the syntactic subject and the proto-agent of a clause. That is, it is in the position that passes typical grammatical test for subjecthood, and this argument is the more agentive, the do-er, or the proto-agent of the clause.\n",
    "\n",
    "3. ccomp clausal complement:\n",
    "4. prep prepositional modifier\n",
    "5. pobj object of preposition\n",
    "6. adverbial modifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "\n",
    "|No.|Name|Description|\n",
    "|---|----|-----------|\n",
    "|1|Tokenization|Segmenting text into words, punctuations mark etc|\n",
    "|2|Part-of-speech(POS) Tagging|Assigning word types to tokens like verb or noun|\n",
    "|3|Dependency Parsing|Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.|\n",
    "|4|Lemmatization|Assigning the base forms of words. For example, the lemma of “was” is “be”, and the lemma of “rats” is “rat”.|\n",
    "|5|Sentence Boundary Detection(SBD)|Finding and segmenting individual sentences.|\n",
    "|6|Named Entity Recognition|Labelling named “real-world” objects, like persons, companies or locations.|\n",
    "|7|Entity Linking (EL)|Disambiguating textual entities to unique identifiers in a knowledge base.|\n",
    "|8|Similarity|Comparing words, text spans and documents and how similar they are to each other.|\n",
    "|9|Text Classification|Assigning categories or labels to a whole document, or parts of a document.|\n",
    "|10|Rule-based Matching|Finding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization\n",
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
    "\n",
    "1. Stemming: Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.<br>         popular algorithms: porter'stemmer, Paice/Husk stemmer\n",
    "2. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
    "3. Stemmers use language-specific rules, but they require less knowledge than a lemmatizer, which needs a complete vocabulary and morphological analysis to correctly lemmatize words.\n",
    "4. Lemmatizer are a tool from Natural Language Processing which does full morphological analysis to accurately identify the lemma for each word. Doing full morphological analysis produces at most very modest benefits for retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information extraction is a task of automatically extracting structured information from unstructured and/or semi-structured documents. In most of the cases, this activity concerns processing human language texts by means of NLP.\n",
    "\n",
    "Typical Information Processing task's order of operation:\n",
    "1. Named Entity Recognition (NER)\n",
    "2. Named Entity Linking (NEL)\n",
    "3. Relation Extraction\n",
    "\n",
    "NEL is the task to link entity mentions in text with their corresponding entities in a knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Models:\n",
    "Some of the spacy require trained pipelines to be loaded, which enable spaCy to predict linguistic annotations.\n",
    "Common Components of the trained pipelines:\n",
    "1. Binary weights for the part-of-speech tagger, dependency parser and named entity recognizer to predict those annotations in context.\n",
    "2. Lexical entries in the vocabulary, i.e. words and their context-independent attributes like the shape or spelling.\n",
    "3. Data files like lemmatization rules and lookup tables.\n",
    "4. Word vectors, i.e. multi-dimensional meaning representations of words that let you determine how similar they are to each other.\n",
    "5. Configuration options, like the language and processing pipeline settings and model implementations to use, to put spaCy in the correct state when you load the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let VERB ROOT let\n",
      "'s PRON nsubj 's\n",
      "go VERB ccomp go\n",
      "to ADP prep to\n",
      "the DET det the\n",
      "mall NOUN pobj mall\n",
      "sometime ADV advmod sometime\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Let's go to the mall sometime\") #tokens on parsing\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.lemma_)\n",
    "    \n",
    "#displacy.render(doc, style=\"dep\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advmod adverbial modifier\n"
     ]
    }
   ],
   "source": [
    "exp = lambda x:print(x,spacy.explain(x))\n",
    "exp('advmod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "During processing, spaCy first tokenizes the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language.\n",
    "\n",
    "First, the raw text is split on whitespace characters, similar to text.split(' '). Then, the tokenizer processes the text from left to right. On each substring, it performs two checks:\n",
    "\n",
    "1. Does the substring match a tokenizer exception rule? For example, “don’t” does not contain whitespace, but should be split into two tokens, “do” and “n’t”, while “U.K.” should always remain one token.\n",
    "\n",
    "2. Can a prefix, suffix or infix be split off? For example punctuation like commas, periods, hyphens or quotes\n",
    "\n",
    "\n",
    "### POS And Tagging\n",
    "After tokenization, spaCy can parse and tag a given Doc. This is where the trained pipeline and its statistical models come in, which enable spaCy to make predictions of which tag or label most likely applies in this context. A trained component includes binary data that is produced by showing a system enough examples for it to make predictions that generalize across the language.\n",
    "\n",
    "### Named Entity\n",
    "A named entity is a “real-world object” that’s assigned a name – for example, a person, a country, a product or a book title. spaCy can recognize various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn’t always work perfectly and might need some tuning later, depending on your use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging\n",
      "When ADV\n",
      "Raman PROPN\n",
      "started VERB\n",
      "working VERB\n",
      "on ADP\n",
      "self NOUN\n",
      "- PUNCT\n",
      "driving VERB\n",
      "cars NOUN\n",
      "at ADP\n",
      "Google PROPN\n",
      "in ADP\n",
      "2007 NUM\n",
      ", PUNCT\n",
      "few ADJ\n",
      "people NOUN\n",
      "outside ADP\n",
      "of ADP\n",
      "the DET\n",
      "company NOUN\n",
      "took VERB\n",
      "him PRON\n",
      "seriously ADV\n",
      ". PUNCT\n",
      "Raman 5 10 PERSON\n",
      "2007 61 65 DATE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">When \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Raman\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " started working on self-driving cars at Google in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2007\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", few people outside of the company took him seriously.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"When Raman started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\"\n",
    "\n",
    "\n",
    "doc2= nlp(text)\n",
    "print(\"POS Tagging\")\n",
    "for token in doc2:\n",
    "    print(token.text, token.pos_)\n",
    "for ent in doc2.ents: # gets named entities \n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "#displacy.render(doc2, style=\"dep\") \n",
    "displacy.render(doc2, style=\"ent\") #entities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors and Similarity\n",
    "Similarity is determined by comparing word vectors or “word embeddings”, multi-dimensional meaning representations of a word. Word vectors can be generated using an algorithm like word2vec.\n",
    "To make them compact and fast, spaCy’s small pipeline packages (all packages that end in sm) don’t ship with word vectors, and only include context-sensitive tensors.\n",
    "\n",
    "\n",
    "Word vectors are stored in a big table in the model and when you look up `cat`, you always get the same vector from this table.\n",
    "\n",
    "The context-sensitive tensors are `dense feature vectors` computed by the models in the pipeline while analyzing the text. You will get different vectors for cat in different texts. If you use en_core_web_sm, the token cat in I have a cat will not have the same vector as in The cat is black. Having the context-sensitive tensors available when the model doesn't include word vectors lets the similarity functions work to some degree, but the results are very different than with word vectors.\n",
    "\n",
    "For most purposes, you probably want to use the _md or _lg model with word vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 7.0336733 False\n",
      "cat True 6.6808186 False\n",
      "eat True 6.9718246 False\n",
      "banana True 6.700014 False\n",
      "ksksdd False 0.0 True\n",
      "car True 7.149045 False\n",
      "space True 6.336554 False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "tokens = nlp(\"dog cat eat banana ksksdd car space\")\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n",
    "#     print('[', end='')\n",
    "#     for n in token.vector:\n",
    "#         print(n, end=',')\n",
    "#     print(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity between `dogs and cats are enemies` and `cats and dogs are friends` is 0.9462085072523121\n"
     ]
    }
   ],
   "source": [
    "doc11 = nlp(\"dogs and cats are enemies\")\n",
    "doc12 = nlp(\"cats and dogs are friends\") #gives high score\n",
    "print('similarity between `%s` and `%s` is %s'%(doc11.text, doc12.text,doc11.similarity(doc12)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation(s) From similarity results\n",
    "\n",
    "1. There’s no objective definition of similarity. Whether “I like burgers” and “I like pasta” is similar depends on your application. Both talk about food preferences, which makes them very similar – but if you’re analyzing mentions of food, those sentences are pretty dissimilar, because they talk about very different foods.\n",
    "2. The similarity of Doc and Span objects defaults to the average of the token vectors. This means that the vector for “fast food” is the average of the vectors for “fast” and “food”, which isn’t necessarily representative of the phrase “fast food”.\n",
    "3. Vector averaging means that the vector of multiple tokens is insensitive to the order of the words. Two documents expressing the same meaning with dissimilar wording will return a lower similarity score than two documents that happen to contain the same words while expressing different meanings.\n",
    "\n",
    "The similarity function by default depends more on words than on intent/context etc.\n",
    "\n",
    "## PIPELINES\n",
    "Calling nlp on a text, spaCy first tokenizes the text to produce a Doc object. The Doc is then processed in several different steps – this is also referred to as the processing pipeline. The pipeline used by the trained pipelines typically include a tagger, a lemmatizer, a parser and an entity recognizer.\n",
    "\n",
    "The processing pipeline consists of one or more pipeline components that are called on the Doc in order. They can contain a statistical model and trained weights, or only make rule-based modifications to the Doc. They can either be \n",
    "<img src=\"https://spacy.io/pipeline-fde48da9b43661abcdf62ab70a546d71.svg\">\n",
    "Various components of pipelines:\n",
    "\n",
    "|No.|Name|Component|Creates|ABOUT|\n",
    "|---|----|---------|-------|-----|\n",
    "|1|tokenizer|Tokenizer|Doc|Text into Tokens|\n",
    "|2|tagger   |Tagger   |Token.tag|Assigns POS tags|\n",
    "|3|parser   |Dependency Parser|Token.head, Token.dep, Doc.sents, Doc.noun_chunks|Assign Dependency Labels       |\n",
    "|4|ner      |Entity Recognizer|Doc.ents, Token.ent_iob, Token.ent_type          |Detect and labels named entitie|\n",
    "|4.1|-------|EntityRuler      |---|Add entity spans to the Doc using token-based rules or exact phrase matches.|\n",
    "|5|lemmatizer|Lemmatizer      |Token.lemma                                      |Assign base forms              |\n",
    "|6|textcat   |TextCategorizer |Doc.cats                                         |Assign document labels.        |\n",
    "|7|custom    |custom components|                                                |                               |\n",
    "|8|morphologizer|---|Morphologizer|Predict morphological features and coarse-grained part-of-speech tags.|\n",
    "|9|sentence recognizer|SentenceRecognizer|---|Predicts sentence boundaries|\n",
    "|10|sentencizer|Sentencizer|---|Implement rule-based sentence boundary detection that doesn’t require the dependency parse|\n",
    "|11|---|TextCategorizer|----|Predict categories or labels over the whole document.|\n",
    "|12|---|Tok2Vec|---|Apply a “token-to-vector” model and set its outputs.|\n",
    "|13|---|TrainablePipe|---|Class that all trainable pipeline components inherit from.|\n",
    "|14|---|Transformer|---|Use a transformer model and set its outputs.|\n",
    "\n",
    "\n",
    "#### Order Of Pipeline\n",
    "The statistical components like the tagger or parser are typically independent and don’t share any data between each other. For example, the named entity recognizer doesn’t use any features set by the tagger and parser, and so on. This means that you can swap them, or remove single components from the pipeline without affecting the others. However, components may share a “token-to-vector” component like Tok2Vec or Transformer.\n",
    "\n",
    "-  Custom components may also depend on annotations set by other components. For example, a custom lemmatizer may need the part-of-speech tags assigned, so it’ll only work if it’s added after the tagger.\n",
    "- The parser will respect pre-defined sentence boundaries, so if a previous component in the pipeline sets them, its dependency predictions may be different.\n",
    "- EntityLinker which resolves named entities to knowledge base IDs, should be preceded by the component that recognizes the entities such as EntitiyRecognizer.\n",
    "\n",
    "#### Tokenizer \n",
    "The tokenizer is a “special” component and isn’t part of the regular pipeline. It also doesn’t show up in nlp.pipe_names. The reason is that there can only really be one tokenizer, and while all other pipeline components take a Doc and return it, the tokenizer takes a `string` of text and turns it into a `Doc`.\n",
    "\n",
    "#### Matchers\n",
    "Matchers help you find and extract information from Doc objects based on match patterns describing the sequences you’re looking for. A matcher operates on a Doc and gives you access to the matched tokens in context.\n",
    "\n",
    "|no.|Name|Description|\n",
    "|---|----|-----------|\n",
    "|1|DependencyMatcher|Match sequences of tokens based on dependency trees using Semgrex operators.|\n",
    "|2|Matcher|Matches sequence of tokens based on pattern rules, similar to regular expression|\n",
    "|3|PhraseMatcher|Match sequences of tokens based on phrases.|\n",
    "\n",
    "#### Container Objects\n",
    "\n",
    "|no.|Name|Description|\n",
    "|---|----|-----------|\n",
    "|1|Corpus|Class for managing annotated corpora for training and evaluation data.|\n",
    "|2|KnowledgeBase|Storage for entities and aliases of a knowledge base for entity linking.|\n",
    "|3|Lookups|Container for convenient access to large lookup tables and dictionaries.|\n",
    "|4|MorphAnalysis|A morphological analysis.|\n",
    "|5|Morphology|Store morphological analyses and map them to and from hash values.|\n",
    "|6|Scorer|Compute evalution scores|\n",
    "|7|StringStore|Map strings to and from hash values.|\n",
    "|8|Vectors|Container class for vector data keyed by string|\n",
    "|9|Vocab|The shared vocabulary that stores strings and gives you access to Lexeme objects.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture \n",
    "Central Data Structures:\n",
    "1. Language Class (TEXT to DOC conv)<br>\n",
    "   It is used to process the text and turn it into `doc`.\n",
    "2. Vocab Object (String Stores, Lexical Attributes and word vectors)<br>\n",
    "   Strings, word vectors and lexical attributes are stored in vocab to avoid keep their multiple copies.By centralizing strings, word vectors and lexical attributes in the Vocab, we avoid storing multiple copies of this data. This saves memory, and ensures there’s a single source of truth\n",
    "3. Doc <br>\n",
    "   The Doc object owns the sequence of tokens and all their annotations.\n",
    "   \n",
    "   <img src=\"https://spacy.io/architecture-415624fc7d149ec03f2736c4aa8b8f3c.svg\" length=500 width=500>\n",
    " \n",
    "Doc object owns the data, and Span and Token are views that point into it. \n",
    "The Doc object is constructed by the Tokenizer, and then modified in place by the components of the pipeline. \n",
    "\n",
    "The Language object coordinates these components. It takes raw text and sends it through the pipeline, returning an annotated document. It also orchestrates training and serialization.\n",
    "\n",
    "__orchestrate__: plan or coordinate the elements of (a situation) to produce a desired effect, especially surreptitiously.\n",
    "\n",
    "Container Objects:\n",
    "\n",
    "|Sno.|Name|Description|\n",
    "|----|----|-----------|\n",
    "|1|__Doc__|A container for accessing linguistic annotations.(Tokens and all annotations)|\n",
    "|2|DocBin|Doc object for efficient serialization. Used for training data mostly|\n",
    "|3|Example|Collection of training annotations, containing two `Doc`obs: reference data and predictions|\n",
    "|4|__Lexeme__|An entry in the vocabulary. It’s a word type with no context, as opposed to a word token. It therefore has no part-of-speech tag, dependency parse etc.|\n",
    "|5|__Language__|Processing class that turns text into Doc objects. Different languages implement their own subclasses of it. The variable is typically called nlp|\n",
    "|6|Span|A slice from a `Doc` object|\n",
    "|7|SpanGroups|A named collection of spans belonging to a Doc|\n",
    "|8|Token|A singleword, punctuation symbol, whitespace, etc.|\n",
    "\n",
    "\n",
    "Referencing Objects And Documents:\n",
    "Whenever possible, spaCy tries to store data in a vocabulary, the Vocab, that will be shared by multiple documents.\n",
    "To save memory, spaCy also encodes all strings to hash values.\n",
    "Internally, spaCy only “speaks” in hash values.\n",
    "\n",
    "All strings are encoded, the entries in the vocabulary don’t need to include the word text themselves. Instead, they can look it up in the StringStore via its hash value. Each entry in the vocabulary, also called Lexeme, contains the context-independent information about a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when advmod  advmod\n",
      "this det  det\n",
      "happend ROOT  ROOT\n",
      "that nsubj  nsubj\n",
      "happened relcl  relcl\n",
      "and cc  cc\n",
      "other conj  conj\n",
      "than prep  prep\n",
      "this pobj  pobj\n",
      "some nsubj  nsubj\n",
      "of prep  prep\n",
      "that pobj  pobj\n",
      "did aux  aux\n",
      "n't neg  neg\n",
      "happen ROOT  ROOT\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"when this happend that happened and other than this some of that didn't happen\")\n",
    "for token in doc:\n",
    "    print(token.text,token.dep_, token.ent_id_,token.dep_)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure each value is unique, spaCy uses a hash function to calculate the hash based on the word string. This also means that the hash for “coffee” will always be the same, no matter which pipeline you’re using or how you’ve configured spaCy.\n",
    "However, hashes cannot be reversed and there’s no way to resolve 3197928453018144401 back to “coffee”. All spaCy can do is look it up in the vocabulary.\n",
    "\n",
    "\n",
    "### Serialization\n",
    "`to_bytes` or `from_bytes`\n",
    "\n",
    "`to_disk` or `from_disk`\n",
    "\n",
    "modifying the pipeline, vocabulary, vectors and entities, or made updates to the component models, save your progress – for example, everything that’s in your nlp object.his process is called serialization. spaCy comes with built-in serialization methods and supports the Pickle protocol.\n",
    "\n",
    "### Training\n",
    "SpaCy’s tagger, parser, text categorizer and many other components are powered by statistical models. Every “decision” these components make is a prediction based on the model’s current weight values. The weight values are estimated based on examples the model has seen during training.\n",
    "<img src=\"https://spacy.io/training-2bc0e13c59784440ecb60ffa82c0783d.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_dev",
   "language": "python",
   "name": "tf_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
